{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install gensim\n",
    "# !pip install nltk\n",
    "# !pip install sklearn\n",
    "# !pip install numpy\n",
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1a: Political Bias Modeling\n",
    "\n",
    "First we want to build a model of political bias using features that will be available in our primary dataset. We'll import the Spinde political bias dataset and select the article text and bias rating columns. Then, we'll vectorize the article text and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jnsal\\anaconda3\\envs\\TestEnv1\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#First, we'll import the political bias dataset. We'll only keep the article body text ('article') and bias type ('type').\n",
    "pb_spinde = pd.read_excel('assets\\pb_spinde.xlsx')\n",
    "pb_reduced = pb_spinde[['article','type']]\n",
    "pb_reduced = pb_reduced.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll replace the text labels with numbers.\n",
    "pb_reduced['type'] = pb_reduced.type.replace({'center':0,'left':-1,'right':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we'll do a simple nltk word tokenization on the article text.\n",
    "pb_reduced['tokens'] = pb_reduced.article.apply(lambda y: [str.lower(x) for x in word_tokenize(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll train the Word2Vec model on our text tokens.\n",
    "wv_mod = Word2Vec(pb_reduced['tokens'], seed = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll extract the vectors from the model...\n",
    "vectors = wv_mod.wv\n",
    "#...and since each word is a vector of 100 numbers, we'll take the mean of all word vectors in a given article \n",
    "#to represent the article as a whole\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in pb_reduced.tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we'll train a Random Forest classifier on the vectorized text to predict article bias.\n",
    "X_train, X_test, y_train, y_test = train_test_split(vec_frame, pb_reduced.type, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1b: Applying the Model\n",
    "\n",
    "Now, we want to predict the political bias of the target fake news dataset. We'll save these predictions as probabilities, which we'll use as additional features for clustering and trustworthiness prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dataset and drop any rows with NA values\n",
    "fn_kag = pd.read_csv(r'assets\\fn_kagg_train.csv')\n",
    "fn_kag = fn_kag.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize article body text\n",
    "fn_kag_tok = fn_kag.copy()\n",
    "fn_kag_tok['text_tokens'] = fn_kag_tok.text.apply(lambda y: [str.lower(x) for x in word_tokenize(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some articles have very few words, so we'll drop any rows with fewer than 30 tokens.\n",
    "fn_kag_tok['tmp'] = fn_kag_tok['text_tokens'].apply(lambda x: len(x))\n",
    "fn_kag_tok = fn_kag_tok[fn_kag_tok['tmp']>30]\n",
    "fn_kag_tok = fn_kag_tok.drop(columns='tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll apply the Word2Vec model we generated above to our tokens to vectorize the text.\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in fn_kag_tok.text_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we apply the Random Forest classifier to our vectorized text and save out the predicted probabilities.\n",
    "preds = pd.DataFrame(clf.predict_proba(vec_frame), columns=['dem_bias','neutral','rep_bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>dem_bias</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rep_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>[house, dem, aide, :, we, didn, ’, t, even, se...</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ever, get, the, feeling, your, life, circles,...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[why, the, truth, might, get, you, fired, octo...</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>[videos, 15, civilians, killed, in, single, us...</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>[print, an, iranian, woman, has, been, sentenc...</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17939</th>\n",
       "      <td>20795</td>\n",
       "      <td>Rapper T.I.: Trump a ’Poster Child For White S...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Rapper T. I. unloaded on black celebrities who...</td>\n",
       "      <td>0</td>\n",
       "      <td>[rapper, t., i., unloaded, on, black, celebrit...</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17940</th>\n",
       "      <td>20796</td>\n",
       "      <td>N.F.L. Playoffs: Schedule, Matchups and Odds -...</td>\n",
       "      <td>Benjamin Hoffman</td>\n",
       "      <td>When the Green Bay Packers lost to the Washing...</td>\n",
       "      <td>0</td>\n",
       "      <td>[when, the, green, bay, packers, lost, to, the...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17941</th>\n",
       "      <td>20797</td>\n",
       "      <td>Macy’s Is Said to Receive Takeover Approach by...</td>\n",
       "      <td>Michael J. de la Merced and Rachel Abrams</td>\n",
       "      <td>The Macy’s of today grew from the union of sev...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, macy, ’, s, of, today, grew, from, the, ...</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17942</th>\n",
       "      <td>20798</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>Alex Ansary</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>1</td>\n",
       "      <td>[nato, ,, russia, to, hold, parallel, exercise...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17943</th>\n",
       "      <td>20799</td>\n",
       "      <td>What Keeps the F-35 Alive</td>\n",
       "      <td>David Swanson</td>\n",
       "      <td>David Swanson is an author, activist, journa...</td>\n",
       "      <td>1</td>\n",
       "      <td>[david, swanson, is, an, author, ,, activist, ...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17944 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2          2                  Why the Truth Might Get You Fired   \n",
       "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4          4  Iranian woman jailed for fictional unpublished...   \n",
       "...      ...                                                ...   \n",
       "17939  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
       "17940  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
       "17941  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
       "17942  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
       "17943  20799                          What Keeps the F-35 Alive   \n",
       "\n",
       "                                          author  \\\n",
       "0                                  Darrell Lucus   \n",
       "1                                Daniel J. Flynn   \n",
       "2                             Consortiumnews.com   \n",
       "3                                Jessica Purkiss   \n",
       "4                                 Howard Portnoy   \n",
       "...                                          ...   \n",
       "17939                              Jerome Hudson   \n",
       "17940                           Benjamin Hoffman   \n",
       "17941  Michael J. de la Merced and Rachel Abrams   \n",
       "17942                                Alex Ansary   \n",
       "17943                              David Swanson   \n",
       "\n",
       "                                                    text  label  \\\n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
       "1      Ever get the feeling your life circles the rou...      0   \n",
       "2      Why the Truth Might Get You Fired October 29, ...      1   \n",
       "3      Videos 15 Civilians Killed In Single US Airstr...      1   \n",
       "4      Print \\nAn Iranian woman has been sentenced to...      1   \n",
       "...                                                  ...    ...   \n",
       "17939  Rapper T. I. unloaded on black celebrities who...      0   \n",
       "17940  When the Green Bay Packers lost to the Washing...      0   \n",
       "17941  The Macy’s of today grew from the union of sev...      0   \n",
       "17942  NATO, Russia To Hold Parallel Exercises In Bal...      1   \n",
       "17943    David Swanson is an author, activist, journa...      1   \n",
       "\n",
       "                                             text_tokens  dem_bias  neutral  \\\n",
       "0      [house, dem, aide, :, we, didn, ’, t, even, se...      0.42     0.07   \n",
       "1      [ever, get, the, feeling, your, life, circles,...      0.33     0.02   \n",
       "2      [why, the, truth, might, get, you, fired, octo...      0.52     0.03   \n",
       "3      [videos, 15, civilians, killed, in, single, us...      0.24     0.31   \n",
       "4      [print, an, iranian, woman, has, been, sentenc...      0.46     0.25   \n",
       "...                                                  ...       ...      ...   \n",
       "17939  [rapper, t., i., unloaded, on, black, celebrit...      0.23     0.03   \n",
       "17940  [when, the, green, bay, packers, lost, to, the...      0.27     0.35   \n",
       "17941  [the, macy, ’, s, of, today, grew, from, the, ...      0.51     0.17   \n",
       "17942  [nato, ,, russia, to, hold, parallel, exercise...      0.30     0.31   \n",
       "17943  [david, swanson, is, an, author, ,, activist, ...      0.56     0.13   \n",
       "\n",
       "       rep_bias  \n",
       "0          0.51  \n",
       "1          0.65  \n",
       "2          0.45  \n",
       "3          0.45  \n",
       "4          0.29  \n",
       "...         ...  \n",
       "17939      0.74  \n",
       "17940      0.38  \n",
       "17941      0.32  \n",
       "17942      0.39  \n",
       "17943      0.31  \n",
       "\n",
       "[17944 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally, we'll rejoin the predictions to the original dataset.\n",
    "fn_kag_reduced = fn_kag_tok.copy().reset_index(drop=True)\n",
    "fn_kag_reduced = fn_kag_reduced.join(preds)\n",
    "fn_kag_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have bias predictions for each article in our fake news dataset. We could follow a similar procedure for additional features (e.g. sentiment analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 2: Clustering\n",
    "\n",
    "Once we have all the features we want, we'll do unsupervised clustering. Ideally we'd want to do some evaluations to find an ideal number of clusters, but for now we'll just go with 4.\n",
    "\n",
    "We'll need to re-vectorize the text, as the political bias vectors won't work here. Also, we'd probably want to vectorize *both* headline and article body, but for now I'll just vectorize the article body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we already have the tokenized text from above, we can just go ahead and train the new Word2Vec model on those tokens.\n",
    "wv_mod = Word2Vec(fn_kag_reduced['text_tokens'], seed = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again we'll extract and average the word vectors.\n",
    "vectors = wv_mod.wv\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in fn_kag_reduced.text_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll join the new word vectors with the bias estimates we generate above.\n",
    "all_feat_df = vec_frame.join(fn_kag_reduced).drop(columns=['id','title','author','text','label','text_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jnsal\\anaconda3\\envs\\TestEnv1\\lib\\site-packages\\sklearn\\utils\\validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Finally we'll build our clustering model...\n",
    "cls = KMeans(4, random_state=RANDOM_SEED).fit(all_feat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jnsal\\anaconda3\\envs\\TestEnv1\\lib\\site-packages\\sklearn\\utils\\validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>dem_bias</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rep_bias</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.047740</td>\n",
       "      <td>-0.062067</td>\n",
       "      <td>0.010142</td>\n",
       "      <td>0.023733</td>\n",
       "      <td>0.027136</td>\n",
       "      <td>0.011726</td>\n",
       "      <td>-0.017329</td>\n",
       "      <td>-0.025544</td>\n",
       "      <td>0.027321</td>\n",
       "      <td>-0.041113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049923</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>-0.057304</td>\n",
       "      <td>-0.037434</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.011955</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.038732</td>\n",
       "      <td>-0.062580</td>\n",
       "      <td>-0.000968</td>\n",
       "      <td>0.032428</td>\n",
       "      <td>0.034728</td>\n",
       "      <td>0.006141</td>\n",
       "      <td>-0.037084</td>\n",
       "      <td>-0.027762</td>\n",
       "      <td>0.024189</td>\n",
       "      <td>-0.042112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050013</td>\n",
       "      <td>0.020645</td>\n",
       "      <td>-0.053560</td>\n",
       "      <td>-0.046181</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.009854</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.043878</td>\n",
       "      <td>-0.084607</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.029689</td>\n",
       "      <td>0.024617</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>-0.043939</td>\n",
       "      <td>-0.027483</td>\n",
       "      <td>0.038452</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050848</td>\n",
       "      <td>-0.004226</td>\n",
       "      <td>-0.061635</td>\n",
       "      <td>-0.054407</td>\n",
       "      <td>0.019642</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018492</td>\n",
       "      <td>-0.052355</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>-0.019925</td>\n",
       "      <td>0.036960</td>\n",
       "      <td>-0.009948</td>\n",
       "      <td>-0.020988</td>\n",
       "      <td>-0.009709</td>\n",
       "      <td>0.013210</td>\n",
       "      <td>-0.092974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062181</td>\n",
       "      <td>0.004864</td>\n",
       "      <td>-0.071717</td>\n",
       "      <td>-0.040037</td>\n",
       "      <td>0.029040</td>\n",
       "      <td>0.008478</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.045002</td>\n",
       "      <td>-0.040723</td>\n",
       "      <td>-0.001672</td>\n",
       "      <td>-0.007292</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>-0.038173</td>\n",
       "      <td>-0.011006</td>\n",
       "      <td>0.034885</td>\n",
       "      <td>-0.060621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027709</td>\n",
       "      <td>0.008914</td>\n",
       "      <td>-0.074335</td>\n",
       "      <td>-0.041465</td>\n",
       "      <td>0.038758</td>\n",
       "      <td>-0.006983</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17939</th>\n",
       "      <td>0.042028</td>\n",
       "      <td>-0.041074</td>\n",
       "      <td>-0.007991</td>\n",
       "      <td>0.023312</td>\n",
       "      <td>0.040681</td>\n",
       "      <td>0.004353</td>\n",
       "      <td>-0.045735</td>\n",
       "      <td>-0.018476</td>\n",
       "      <td>0.024066</td>\n",
       "      <td>-0.023719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059906</td>\n",
       "      <td>0.010804</td>\n",
       "      <td>-0.047868</td>\n",
       "      <td>-0.021401</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.019536</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17940</th>\n",
       "      <td>0.021898</td>\n",
       "      <td>-0.026946</td>\n",
       "      <td>0.032306</td>\n",
       "      <td>-0.005613</td>\n",
       "      <td>0.034018</td>\n",
       "      <td>-0.001052</td>\n",
       "      <td>-0.012961</td>\n",
       "      <td>-0.041187</td>\n",
       "      <td>-0.004921</td>\n",
       "      <td>-0.088816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064928</td>\n",
       "      <td>0.010397</td>\n",
       "      <td>-0.034544</td>\n",
       "      <td>-0.019263</td>\n",
       "      <td>0.019563</td>\n",
       "      <td>0.041904</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17941</th>\n",
       "      <td>0.011924</td>\n",
       "      <td>-0.052399</td>\n",
       "      <td>0.033022</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>0.010965</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>-0.028792</td>\n",
       "      <td>-0.027073</td>\n",
       "      <td>0.023143</td>\n",
       "      <td>-0.056647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039514</td>\n",
       "      <td>0.020995</td>\n",
       "      <td>-0.045691</td>\n",
       "      <td>-0.024739</td>\n",
       "      <td>0.013343</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17942</th>\n",
       "      <td>0.039743</td>\n",
       "      <td>-0.046810</td>\n",
       "      <td>0.016195</td>\n",
       "      <td>-0.026166</td>\n",
       "      <td>0.025034</td>\n",
       "      <td>-0.005241</td>\n",
       "      <td>-0.046477</td>\n",
       "      <td>-0.026727</td>\n",
       "      <td>0.009661</td>\n",
       "      <td>-0.106481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067306</td>\n",
       "      <td>-0.031781</td>\n",
       "      <td>-0.070297</td>\n",
       "      <td>-0.045169</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>-0.013425</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17943</th>\n",
       "      <td>0.029514</td>\n",
       "      <td>-0.076079</td>\n",
       "      <td>0.023930</td>\n",
       "      <td>0.027409</td>\n",
       "      <td>0.024942</td>\n",
       "      <td>0.018981</td>\n",
       "      <td>-0.044571</td>\n",
       "      <td>-0.034347</td>\n",
       "      <td>0.031353</td>\n",
       "      <td>-0.066803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043935</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>-0.045624</td>\n",
       "      <td>-0.032178</td>\n",
       "      <td>0.010911</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17944 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.047740 -0.062067  0.010142  0.023733  0.027136  0.011726 -0.017329   \n",
       "1      0.038732 -0.062580 -0.000968  0.032428  0.034728  0.006141 -0.037084   \n",
       "2      0.043878 -0.084607  0.001290  0.029689  0.024617  0.012073 -0.043939   \n",
       "3      0.018492 -0.052355  0.016414 -0.019925  0.036960 -0.009948 -0.020988   \n",
       "4      0.045002 -0.040723 -0.001672 -0.007292  0.032992  0.001198 -0.038173   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "17939  0.042028 -0.041074 -0.007991  0.023312  0.040681  0.004353 -0.045735   \n",
       "17940  0.021898 -0.026946  0.032306 -0.005613  0.034018 -0.001052 -0.012961   \n",
       "17941  0.011924 -0.052399  0.033022 -0.000930  0.010965  0.003568 -0.028792   \n",
       "17942  0.039743 -0.046810  0.016195 -0.026166  0.025034 -0.005241 -0.046477   \n",
       "17943  0.029514 -0.076079  0.023930  0.027409  0.024942  0.018981 -0.044571   \n",
       "\n",
       "              7         8         9  ...        94        95        96  \\\n",
       "0     -0.025544  0.027321 -0.041113  ...  0.049923  0.000369 -0.057304   \n",
       "1     -0.027762  0.024189 -0.042112  ...  0.050013  0.020645 -0.053560   \n",
       "2     -0.027483  0.038452 -0.054688  ...  0.050848 -0.004226 -0.061635   \n",
       "3     -0.009709  0.013210 -0.092974  ...  0.062181  0.004864 -0.071717   \n",
       "4     -0.011006  0.034885 -0.060621  ...  0.027709  0.008914 -0.074335   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "17939 -0.018476  0.024066 -0.023719  ...  0.059906  0.010804 -0.047868   \n",
       "17940 -0.041187 -0.004921 -0.088816  ...  0.064928  0.010397 -0.034544   \n",
       "17941 -0.027073  0.023143 -0.056647  ...  0.039514  0.020995 -0.045691   \n",
       "17942 -0.026727  0.009661 -0.106481  ...  0.067306 -0.031781 -0.070297   \n",
       "17943 -0.034347  0.031353 -0.066803  ...  0.043935  0.000403 -0.045624   \n",
       "\n",
       "             97        98        99  dem_bias  neutral  rep_bias  cluster  \n",
       "0     -0.037434  0.021349  0.011955      0.42     0.07      0.51        0  \n",
       "1     -0.046181  0.028000  0.009854      0.33     0.02      0.65        0  \n",
       "2     -0.054407  0.019642  0.001657      0.52     0.03      0.45        2  \n",
       "3     -0.040037  0.029040  0.008478      0.24     0.31      0.45        1  \n",
       "4     -0.041465  0.038758 -0.006983      0.46     0.25      0.29        2  \n",
       "...         ...       ...       ...       ...      ...       ...      ...  \n",
       "17939 -0.021401  0.015917  0.019536      0.23     0.03      0.74        0  \n",
       "17940 -0.019263  0.019563  0.041904      0.27     0.35      0.38        1  \n",
       "17941 -0.024739  0.013343  0.008033      0.51     0.17      0.32        2  \n",
       "17942 -0.045169  0.007362 -0.013425      0.30     0.31      0.39        1  \n",
       "17943 -0.032178  0.010911  0.013660      0.56     0.13      0.31        2  \n",
       "\n",
       "[17944 rows x 104 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#...and add the predicted clusters back into the vector dataframe.\n",
    "all_feat_df['cluster'] = cls.predict(all_feat_df)\n",
    "all_feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Supervised Learning\n",
    "\n",
    "Now that we have all of our features and clusters, and article body text is already vectorized, we can train a classifier to predict whether a given article is misinformation or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We've already done most of the work above, we'll just split up the dataset and build the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_feat_df, fn_kag_reduced.label, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jnsal\\anaconda3\\envs\\TestEnv1\\lib\\site-packages\\sklearn\\utils\\validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jnsal\\anaconda3\\envs\\TestEnv1\\lib\\site-packages\\sklearn\\utils\\validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9127890777375314"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is... a surprisingly good score, which makes me feel like I did something wrong. I probably should have done the train-test split *before* applying the political bias model and doing the clustering... this probably caused some data leakage. But this is probably good enough for me to get started on the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
