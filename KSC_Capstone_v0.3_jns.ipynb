{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install gensim\n",
    "# !pip install nltk\n",
    "# !pip install sklearn\n",
    "# !pip install numpy\n",
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning The Data - dataset from Deep Blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our text data, we are going to apply some of the text pre-processing techniques. Since this cleaning process can go on forever. There's always an exception to every cleaning steps. So, we're going to do this process in a few rounds.\n",
    "\n",
    "**Below are the steps we will be applying to our dataset:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "* Stemming & Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bias=pd.read_excel('assets\\pb_spinde.xlsx')\n",
    "data_bias=data_bias[['article','type']]\n",
    "data_bias.isnull().values.any()  # check if there are any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1700 entries, 0 to 1699\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   article  1600 non-null   object\n",
      " 1   type     1700 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 26.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data_bias.info()  # there are 100 rows has NaN value in column'article'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article    type\n",
       "0  YouTube says no ‘deepfakes’ or ‘birther’ video...  center\n",
       "1  FRISCO, Texas — The increasingly bitter disput...    left\n",
       "2  Speaking to the country for the first time fro...    left\n",
       "3  A professor who teaches climate change classes...   right\n",
       "4  The left has a thing for taking babies hostage...   right"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bias = data_bias.dropna().reset_index(drop=True)  # decide to drop the rows with missing value\n",
    "data_bias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>youtube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frisco texas — the increasingly bitter dispute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>speaking to the country for the first time fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a professor who teaches climate change classes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the left has a thing for taking babies hostage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>the house democrats’ coronavirus recovery bill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>there are many reasons that republicans and co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>a man’s penis becomes a female penis once a ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>as a selfdescribed democratic socialist sen be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>cbs late show host stephen colbert claimed on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article\n",
       "0     youtube says no ‘deepfakes’ or ‘birther’ video...\n",
       "1     frisco texas — the increasingly bitter dispute...\n",
       "2     speaking to the country for the first time fro...\n",
       "3     a professor who teaches climate change classes...\n",
       "4     the left has a thing for taking babies hostage...\n",
       "...                                                 ...\n",
       "1595  the house democrats’ coronavirus recovery bill...\n",
       "1596  there are many reasons that republicans and co...\n",
       "1597  a man’s penis becomes a female penis once a ma...\n",
       "1598  as a selfdescribed democratic socialist sen be...\n",
       "1599  cbs late show host stephen colbert claimed on ...\n",
       "\n",
       "[1600 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_bias.article.apply(round1))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>youtube says no deepfakes or birther videos wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frisco texas  the increasingly bitter dispute ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>speaking to the country for the first time fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a professor who teaches climate change classes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the left has a thing for taking babies hostage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>the house democrats coronavirus recovery bill ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>there are many reasons that republicans and co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>a mans penis becomes a female penis once a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>as a selfdescribed democratic socialist sen be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>cbs late show host stephen colbert claimed on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article\n",
       "0     youtube says no deepfakes or birther videos wi...\n",
       "1     frisco texas  the increasingly bitter dispute ...\n",
       "2     speaking to the country for the first time fro...\n",
       "3     a professor who teaches climate change classes...\n",
       "4     the left has a thing for taking babies hostage...\n",
       "...                                                 ...\n",
       "1595  the house democrats coronavirus recovery bill ...\n",
       "1596  there are many reasons that republicans and co...\n",
       "1597  a mans penis becomes a female penis once a man...\n",
       "1598  as a selfdescribed democratic socialist sen be...\n",
       "1599  cbs late show host stephen colbert claimed on ...\n",
       "\n",
       "[1600 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”—…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_clean.article.apply(round2))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>youtube says deepfakes birther videos toughene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frisco texas increasingly bitter dispute ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>speaking country first time oval office tuesda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>professor teaches climate change classes subje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>left thing taking babies hostage perfect examp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>house democrats coronavirus recovery bill allo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>many reasons republicans conservative activist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>mans penis becomes female penis man declares t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>selfdescribed democratic socialist sen bernie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>cbs late show host stephen colbert claimed tue...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article\n",
       "0     youtube says deepfakes birther videos toughene...\n",
       "1     frisco texas increasingly bitter dispute ameri...\n",
       "2     speaking country first time oval office tuesda...\n",
       "3     professor teaches climate change classes subje...\n",
       "4     left thing taking babies hostage perfect examp...\n",
       "...                                                 ...\n",
       "1595  house democrats coronavirus recovery bill allo...\n",
       "1596  many reasons republicans conservative activist...\n",
       "1597  mans penis becomes female penis man declares t...\n",
       "1598  selfdescribed democratic socialist sen bernie ...\n",
       "1599  cbs late show host stephen colbert claimed tue...\n",
       "\n",
       "[1600 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#in case, if we need to remove more stopwords us code below\n",
    "# stop_words.append('new words')\n",
    "\n",
    "data_clean['article'] = data_clean['article'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[youtube, says, deepfakes, birther, videos, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[frisco, texas, increasingly, bitter, dispute,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[speaking, country, first, time, oval, office,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[professor, teaches, climate, change, classes,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[left, thing, taking, babies, hostage, perfect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>[house, democrats, coronavirus, recovery, bill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>[many, reasons, republicans, conservative, act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>[mans, penis, becomes, female, penis, man, dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>[selfdescribed, democratic, socialist, sen, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>[cbs, late, show, host, stephen, colbert, clai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article\n",
       "0     [youtube, says, deepfakes, birther, videos, to...\n",
       "1     [frisco, texas, increasingly, bitter, dispute,...\n",
       "2     [speaking, country, first, time, oval, office,...\n",
       "3     [professor, teaches, climate, change, classes,...\n",
       "4     [left, thing, taking, babies, hostage, perfect...\n",
       "...                                                 ...\n",
       "1595  [house, democrats, coronavirus, recovery, bill...\n",
       "1596  [many, reasons, republicans, conservative, act...\n",
       "1597  [mans, penis, becomes, female, penis, man, dec...\n",
       "1598  [selfdescribed, democratic, socialist, sen, be...\n",
       "1599  [cbs, late, show, host, stephen, colbert, clai...\n",
       "\n",
       "[1600 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean['article'] = data_clean.article.apply(lambda y: [x for x in word_tokenize(y)])\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process will chops off the ends of words in the hope of achieving this goal correctly most of the time\n",
    "\n",
    "**(this is optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[youtub, say, deepfak, birther, video, toughen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[frisco, texa, increas, bitter, disput, americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[speak, countri, first, time, oval, offic, tue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[professor, teach, climat, chang, class, subje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[left, thing, take, babi, hostag, perfect, exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>[hous, democrat, coronavirus, recoveri, bill, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>[mani, reason, republican, conserv, activist, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>[man, peni, becom, femal, peni, man, declar, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>[selfdescrib, democrat, socialist, sen, berni,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>[cbs, late, show, host, stephen, colbert, clai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article\n",
       "0     [youtub, say, deepfak, birther, video, toughen...\n",
       "1     [frisco, texa, increas, bitter, disput, americ...\n",
       "2     [speak, countri, first, time, oval, offic, tue...\n",
       "3     [professor, teach, climat, chang, class, subje...\n",
       "4     [left, thing, take, babi, hostag, perfect, exa...\n",
       "...                                                 ...\n",
       "1595  [hous, democrat, coronavirus, recoveri, bill, ...\n",
       "1596  [mani, reason, republican, conserv, activist, ...\n",
       "1597  [man, peni, becom, femal, peni, man, declar, t...\n",
       "1598  [selfdescrib, democrat, socialist, sen, berni,...\n",
       "1599  [cbs, late, show, host, stephen, colbert, clai...\n",
       "\n",
       "[1600 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "data_clean_stemmed = data_clean.copy()\n",
    "\n",
    "data_clean_stemmed['article'] = data_clean_stemmed['article'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
    "data_clean_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this process, we use a vocabulary and morphological analysis of words which is aiming to remove inflectional endings only and to return the base or dictionary form of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[youtube, say, deepfakes, birther, video, toug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[frisco, texas, increasingly, bitter, dispute,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[speaking, country, first, time, oval, office,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[professor, teach, climate, change, class, sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[left, thing, taking, baby, hostage, perfect, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>[house, democrat, coronavirus, recovery, bill,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>[many, reason, republican, conservative, activ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>[man, penis, becomes, female, penis, man, decl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>[selfdescribed, democratic, socialist, sen, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>[cbs, late, show, host, stephen, colbert, clai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article\n",
       "0     [youtube, say, deepfakes, birther, video, toug...\n",
       "1     [frisco, texas, increasingly, bitter, dispute,...\n",
       "2     [speaking, country, first, time, oval, office,...\n",
       "3     [professor, teach, climate, change, class, sub...\n",
       "4     [left, thing, taking, baby, hostage, perfect, ...\n",
       "...                                                 ...\n",
       "1595  [house, democrat, coronavirus, recovery, bill,...\n",
       "1596  [many, reason, republican, conservative, activ...\n",
       "1597  [man, penis, becomes, female, penis, man, decl...\n",
       "1598  [selfdescribed, democratic, socialist, sen, be...\n",
       "1599  [cbs, late, show, host, stephen, colbert, clai...\n",
       "\n",
       "[1600 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "data_clean_lemma = data_clean.copy()\n",
    "\n",
    "data_clean_lemma['article']= data_clean_lemma['article'].apply(lemmatize_text)\n",
    "data_clean_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[youtube, say, deepfakes, birther, video, toug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[frisco, texas, increasingly, bitter, dispute,...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[speaking, country, first, time, oval, office,...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[professor, teach, climate, change, class, sub...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[left, thing, taking, baby, hostage, perfect, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>[house, democrat, coronavirus, recovery, bill,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>[many, reason, republican, conservative, activ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>[man, penis, becomes, female, penis, man, decl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>[selfdescribed, democratic, socialist, sen, be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>[cbs, late, show, host, stephen, colbert, clai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article  type\n",
       "0     [youtube, say, deepfakes, birther, video, toug...     0\n",
       "1     [frisco, texas, increasingly, bitter, dispute,...    -1\n",
       "2     [speaking, country, first, time, oval, office,...    -1\n",
       "3     [professor, teach, climate, change, class, sub...     1\n",
       "4     [left, thing, taking, baby, hostage, perfect, ...     1\n",
       "...                                                 ...   ...\n",
       "1595  [house, democrat, coronavirus, recovery, bill,...     1\n",
       "1596  [many, reason, republican, conservative, activ...    -1\n",
       "1597  [man, penis, becomes, female, penis, man, decl...     1\n",
       "1598  [selfdescribed, democratic, socialist, sen, be...     1\n",
       "1599  [cbs, late, show, host, stephen, colbert, clai...     1\n",
       "\n",
       "[1600 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleanned = data_clean_lemma.copy()\n",
    "\n",
    "data_bias['type'] = data_bias.type.replace({'center':0,'left':-1,'right':1}) # replace the text labels with numbers\n",
    "data_cleanned['type'] = data_bias['type']  # combine with the cleaned dataset\n",
    "data_cleanned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This data cleaning: **text pre-processing step** could go on for a while, but we are going to stop for now and try it in the modeling part. After that, if we see that the results don't make sense or could be improved, we will come back and make more edits\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming \n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams (such as 'thank you' into one term)\n",
    "* Deal with typos\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_cleanned.to_pickle(\"data_cleanned_corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing on the Model part (copied from Jame's code)\n",
    "## Part 1a: Political Bias Modeling\n",
    "\n",
    "First we want to build a model of political bias using features that will be available in our primary dataset. We'll import the Spinde political bias dataset and select the article text and bias rating columns. Then, we'll vectorize the article text and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>type</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>0</td>\n",
       "      <td>[youtube, say, deepfakes, birther, video, toug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[frisco, texas, increasingly, bitter, dispute,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[speaking, country, first, time, oval, office,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>1</td>\n",
       "      <td>[professor, teach, climate, change, class, sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>1</td>\n",
       "      <td>[left, thing, taking, baby, hostage, perfect, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>The House Democrats’ coronavirus recovery bill...</td>\n",
       "      <td>1</td>\n",
       "      <td>[house, democrat, coronavirus, recovery, bill,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>There are many reasons that Republicans and co...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[many, reason, republican, conservative, activ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>A man’s penis becomes a female penis once a ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>[man, penis, becomes, female, penis, man, decl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>As a self-described Democratic socialist, Sen....</td>\n",
       "      <td>1</td>\n",
       "      <td>[selfdescribed, democratic, socialist, sen, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>CBS Late Show host Stephen Colbert claimed on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[cbs, late, show, host, stephen, colbert, clai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article  type  \\\n",
       "0     YouTube says no ‘deepfakes’ or ‘birther’ video...     0   \n",
       "1     FRISCO, Texas — The increasingly bitter disput...    -1   \n",
       "2     Speaking to the country for the first time fro...    -1   \n",
       "3     A professor who teaches climate change classes...     1   \n",
       "4     The left has a thing for taking babies hostage...     1   \n",
       "...                                                 ...   ...   \n",
       "1595  The House Democrats’ coronavirus recovery bill...     1   \n",
       "1596  There are many reasons that Republicans and co...    -1   \n",
       "1597  A man’s penis becomes a female penis once a ma...     1   \n",
       "1598  As a self-described Democratic socialist, Sen....     1   \n",
       "1599  CBS Late Show host Stephen Colbert claimed on ...     1   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [youtube, say, deepfakes, birther, video, toug...  \n",
       "1     [frisco, texas, increasingly, bitter, dispute,...  \n",
       "2     [speaking, country, first, time, oval, office,...  \n",
       "3     [professor, teach, climate, change, class, sub...  \n",
       "4     [left, thing, taking, baby, hostage, perfect, ...  \n",
       "...                                                 ...  \n",
       "1595  [house, democrat, coronavirus, recovery, bill,...  \n",
       "1596  [many, reason, republican, conservative, activ...  \n",
       "1597  [man, penis, becomes, female, penis, man, decl...  \n",
       "1598  [selfdescribed, democratic, socialist, sen, be...  \n",
       "1599  [cbs, late, show, host, stephen, colbert, clai...  \n",
       "\n",
       "[1600 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb_reduced = data_bias.copy()\n",
    "pb_reduced['tokens'] = data_cleanned['article']\n",
    "pb_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll train the Word2Vec model on our text tokens.\n",
    "wv_mod = Word2Vec(pb_reduced['tokens'], seed = RANDOM_SEED)\n",
    "wv_mod.save(\"models/pb_w2v_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll extract the vectors from the model...\n",
    "vectors = wv_mod.wv\n",
    "#...and since each word is a vector of 100 numbers, we'll take the mean of all word vectors in a given article \n",
    "#to represent the article as a whole\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in pb_reduced.tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we'll train a Random Forest classifier on the vectorized text to predict article bias.\n",
    "X_train, X_test, y_train, y_test = train_test_split(vec_frame, pb_reduced.type, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "clf.fit(X_train, y_train)\n",
    "filename = \"models/pb_classifier_model.pkl\"\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning The Data - dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_news=pd.read_csv('assets/fn_kagg_train.csv')\n",
    "data_news.isnull().values.any()  # check if there are any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title     True\n",
       "author    True\n",
       "text      True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_news.isna().any()[lambda x: x] # check which column has missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows 20800\n",
      "Number of title with missing value 558\n",
      "Number of author with missing value 1957\n",
      "Number of text with missing value 39\n"
     ]
    }
   ],
   "source": [
    "print ('Total number of rows',  len(data_news))\n",
    "print('Number of title with missing value', data_news['title'].isna().sum())\n",
    "print('Number of author with missing value', data_news['author'].isna().sum())\n",
    "print('Number of text with missing value', data_news['text'].isna().sum())\n",
    "\n",
    "# it appears that the rows with miss value only has a small number, decide to remove\n",
    "data_news = data_news.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18280</th>\n",
       "      <td>20795</td>\n",
       "      <td>Rapper T.I.: Trump a ’Poster Child For White S...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Rapper T. I. unloaded on black celebrities who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18281</th>\n",
       "      <td>20796</td>\n",
       "      <td>N.F.L. Playoffs: Schedule, Matchups and Odds -...</td>\n",
       "      <td>Benjamin Hoffman</td>\n",
       "      <td>When the Green Bay Packers lost to the Washing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18282</th>\n",
       "      <td>20797</td>\n",
       "      <td>Macy’s Is Said to Receive Takeover Approach by...</td>\n",
       "      <td>Michael J. de la Merced and Rachel Abrams</td>\n",
       "      <td>The Macy’s of today grew from the union of sev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18283</th>\n",
       "      <td>20798</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>Alex Ansary</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18284</th>\n",
       "      <td>20799</td>\n",
       "      <td>What Keeps the F-35 Alive</td>\n",
       "      <td>David Swanson</td>\n",
       "      <td>David Swanson is an author, activist, journa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18285 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2          2                  Why the Truth Might Get You Fired   \n",
       "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4          4  Iranian woman jailed for fictional unpublished...   \n",
       "...      ...                                                ...   \n",
       "18280  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
       "18281  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
       "18282  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
       "18283  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
       "18284  20799                          What Keeps the F-35 Alive   \n",
       "\n",
       "                                          author  \\\n",
       "0                                  Darrell Lucus   \n",
       "1                                Daniel J. Flynn   \n",
       "2                             Consortiumnews.com   \n",
       "3                                Jessica Purkiss   \n",
       "4                                 Howard Portnoy   \n",
       "...                                          ...   \n",
       "18280                              Jerome Hudson   \n",
       "18281                           Benjamin Hoffman   \n",
       "18282  Michael J. de la Merced and Rachel Abrams   \n",
       "18283                                Alex Ansary   \n",
       "18284                              David Swanson   \n",
       "\n",
       "                                                    text  label  \n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1      Ever get the feeling your life circles the rou...      0  \n",
       "2      Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3      Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4      Print \\nAn Iranian woman has been sentenced to...      1  \n",
       "...                                                  ...    ...  \n",
       "18280  Rapper T. I. unloaded on black celebrities who...      0  \n",
       "18281  When the Green Bay Packers lost to the Washing...      0  \n",
       "18282  The Macy’s of today grew from the union of sev...      0  \n",
       "18283  NATO, Russia To Hold Parallel Exercises In Bal...      1  \n",
       "18284    David Swanson is an author, activist, journa...      1  \n",
       "\n",
       "[18285 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the same cleaning rounds from previous code\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "* Stemming & Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_news_clean = pd.DataFrame(data_news.text.apply(round1))\n",
    "data_news_clean = pd.DataFrame(data_news_clean.text.apply(round2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More cleaning are required \n",
    "\n",
    "**After examine the 'text' columns, it appears that some characters are encoded with UTF-8 format, and ascii value\n",
    "(such as Ã¼, Ð», â€“.etc)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>house dem aide we didnt even see comeys letter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ever get the feeling your life circles the rou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why the truth might get you fired october   th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>videos  civilians killed in single us airstrik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>print an iranian woman has been sentenced to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18280</th>\n",
       "      <td>rapper t i unloaded on black celebrities who m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18281</th>\n",
       "      <td>when the green bay packers lost to the washing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18282</th>\n",
       "      <td>the macys of today grew from the union of seve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18283</th>\n",
       "      <td>nato russia to hold parallel exercises in balk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18284</th>\n",
       "      <td>david swanson is an author activist journali...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18285 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      house dem aide we didnt even see comeys letter...\n",
       "1      ever get the feeling your life circles the rou...\n",
       "2      why the truth might get you fired october   th...\n",
       "3      videos  civilians killed in single us airstrik...\n",
       "4      print an iranian woman has been sentenced to s...\n",
       "...                                                  ...\n",
       "18280  rapper t i unloaded on black celebrities who m...\n",
       "18281  when the green bay packers lost to the washing...\n",
       "18282  the macys of today grew from the union of seve...\n",
       "18283  nato russia to hold parallel exercises in balk...\n",
       "18284    david swanson is an author activist journali...\n",
       "\n",
       "[18285 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a third round of cleaning\n",
    "\n",
    "'''Remove some UTF-8 format characters and converting the ascii value which missed from previous rounds'''\n",
    "data_news_clean['text'] = data_news_clean['text'].apply(lambda x: x.encode('ascii', 'ignore').decode(\"ascii\",\"ignore\"))\n",
    "data_news_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop, Tokenization, Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this process will run a bit longer since we are doing all three together \n",
    "\n",
    "data_news_clean['text'] = data_news_clean['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "data_news_clean['text'] = data_news_clean.text.apply(lambda y: [x for x in word_tokenize(y)])\n",
    "data_news_lemma = data_news_clean.copy()\n",
    "data_news_lemma['text']= data_news_lemma['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_news_lemma.to_pickle(\"data_news_lemma_corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing on the Model part (copied from Jame's code)\n",
    "## Part 1b: Applying the Model\n",
    "\n",
    "Now, we want to predict the political bias of the target fake news dataset. We'll save these predictions as probabilities, which we'll use as additional features for clustering and trustworthiness prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_kag_tok = data_news.copy()\n",
    "fn_kag_tok['text_tokens'] = data_news_lemma['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>[house, dem, aide, didnt, even, see, comeys, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ever, get, feeling, life, circle, roundabout,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[truth, might, get, fired, october, tension, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>[video, civilian, killed, single, u, airstrike...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>[print, iranian, woman, sentenced, six, year, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18280</th>\n",
       "      <td>20795</td>\n",
       "      <td>Rapper T.I.: Trump a ’Poster Child For White S...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Rapper T. I. unloaded on black celebrities who...</td>\n",
       "      <td>0</td>\n",
       "      <td>[rapper, unloaded, black, celebrity, met, dona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18281</th>\n",
       "      <td>20796</td>\n",
       "      <td>N.F.L. Playoffs: Schedule, Matchups and Odds -...</td>\n",
       "      <td>Benjamin Hoffman</td>\n",
       "      <td>When the Green Bay Packers lost to the Washing...</td>\n",
       "      <td>0</td>\n",
       "      <td>[green, bay, packer, lost, washington, redskin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18282</th>\n",
       "      <td>20797</td>\n",
       "      <td>Macy’s Is Said to Receive Takeover Approach by...</td>\n",
       "      <td>Michael J. de la Merced and Rachel Abrams</td>\n",
       "      <td>The Macy’s of today grew from the union of sev...</td>\n",
       "      <td>0</td>\n",
       "      <td>[macys, today, grew, union, several, great, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18283</th>\n",
       "      <td>20798</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>Alex Ansary</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>1</td>\n",
       "      <td>[nato, russia, hold, parallel, exercise, balka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18284</th>\n",
       "      <td>20799</td>\n",
       "      <td>What Keeps the F-35 Alive</td>\n",
       "      <td>David Swanson</td>\n",
       "      <td>David Swanson is an author, activist, journa...</td>\n",
       "      <td>1</td>\n",
       "      <td>[david, swanson, author, activist, journalist,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2          2                  Why the Truth Might Get You Fired   \n",
       "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4          4  Iranian woman jailed for fictional unpublished...   \n",
       "...      ...                                                ...   \n",
       "18280  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
       "18281  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
       "18282  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
       "18283  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
       "18284  20799                          What Keeps the F-35 Alive   \n",
       "\n",
       "                                          author  \\\n",
       "0                                  Darrell Lucus   \n",
       "1                                Daniel J. Flynn   \n",
       "2                             Consortiumnews.com   \n",
       "3                                Jessica Purkiss   \n",
       "4                                 Howard Portnoy   \n",
       "...                                          ...   \n",
       "18280                              Jerome Hudson   \n",
       "18281                           Benjamin Hoffman   \n",
       "18282  Michael J. de la Merced and Rachel Abrams   \n",
       "18283                                Alex Ansary   \n",
       "18284                              David Swanson   \n",
       "\n",
       "                                                    text  label  \\\n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
       "1      Ever get the feeling your life circles the rou...      0   \n",
       "2      Why the Truth Might Get You Fired October 29, ...      1   \n",
       "3      Videos 15 Civilians Killed In Single US Airstr...      1   \n",
       "4      Print \\nAn Iranian woman has been sentenced to...      1   \n",
       "...                                                  ...    ...   \n",
       "18280  Rapper T. I. unloaded on black celebrities who...      0   \n",
       "18281  When the Green Bay Packers lost to the Washing...      0   \n",
       "18282  The Macy’s of today grew from the union of sev...      0   \n",
       "18283  NATO, Russia To Hold Parallel Exercises In Bal...      1   \n",
       "18284    David Swanson is an author, activist, journa...      1   \n",
       "\n",
       "                                             text_tokens  \n",
       "0      [house, dem, aide, didnt, even, see, comeys, l...  \n",
       "1      [ever, get, feeling, life, circle, roundabout,...  \n",
       "2      [truth, might, get, fired, october, tension, i...  \n",
       "3      [video, civilian, killed, single, u, airstrike...  \n",
       "4      [print, iranian, woman, sentenced, six, year, ...  \n",
       "...                                                  ...  \n",
       "18280  [rapper, unloaded, black, celebrity, met, dona...  \n",
       "18281  [green, bay, packer, lost, washington, redskin...  \n",
       "18282  [macys, today, grew, union, several, great, na...  \n",
       "18283  [nato, russia, hold, parallel, exercise, balka...  \n",
       "18284  [david, swanson, author, activist, journalist,...  \n",
       "\n",
       "[17492 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some articles have very few words, so we'll drop any rows with fewer than 30 tokens.\n",
    "fn_kag_tok['tmp'] = fn_kag_tok['text_tokens'].apply(lambda x: len(x))\n",
    "fn_kag_tok = fn_kag_tok[fn_kag_tok['tmp']>30]\n",
    "fn_kag_tok = fn_kag_tok.drop(columns='tmp')\n",
    "fn_kag_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll apply the Word2Vec model we generated above to our tokens to vectorize the text.\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in fn_kag_tok.text_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we apply the Random Forest classifier to our vectorized text and save out the predicted probabilities.\n",
    "preds = pd.DataFrame(clf.predict_proba(vec_frame), columns=['dem_bias','neutral','rep_bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>dem_bias</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rep_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>[house, dem, aide, didnt, even, see, comeys, l...</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ever, get, feeling, life, circle, roundabout,...</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[truth, might, get, fired, october, tension, i...</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>[video, civilian, killed, single, u, airstrike...</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>[print, iranian, woman, sentenced, six, year, ...</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17487</th>\n",
       "      <td>20795</td>\n",
       "      <td>Rapper T.I.: Trump a ’Poster Child For White S...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Rapper T. I. unloaded on black celebrities who...</td>\n",
       "      <td>0</td>\n",
       "      <td>[rapper, unloaded, black, celebrity, met, dona...</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17488</th>\n",
       "      <td>20796</td>\n",
       "      <td>N.F.L. Playoffs: Schedule, Matchups and Odds -...</td>\n",
       "      <td>Benjamin Hoffman</td>\n",
       "      <td>When the Green Bay Packers lost to the Washing...</td>\n",
       "      <td>0</td>\n",
       "      <td>[green, bay, packer, lost, washington, redskin...</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17489</th>\n",
       "      <td>20797</td>\n",
       "      <td>Macy’s Is Said to Receive Takeover Approach by...</td>\n",
       "      <td>Michael J. de la Merced and Rachel Abrams</td>\n",
       "      <td>The Macy’s of today grew from the union of sev...</td>\n",
       "      <td>0</td>\n",
       "      <td>[macys, today, grew, union, several, great, na...</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17490</th>\n",
       "      <td>20798</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>Alex Ansary</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>1</td>\n",
       "      <td>[nato, russia, hold, parallel, exercise, balka...</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17491</th>\n",
       "      <td>20799</td>\n",
       "      <td>What Keeps the F-35 Alive</td>\n",
       "      <td>David Swanson</td>\n",
       "      <td>David Swanson is an author, activist, journa...</td>\n",
       "      <td>1</td>\n",
       "      <td>[david, swanson, author, activist, journalist,...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17492 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2          2                  Why the Truth Might Get You Fired   \n",
       "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4          4  Iranian woman jailed for fictional unpublished...   \n",
       "...      ...                                                ...   \n",
       "17487  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
       "17488  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
       "17489  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
       "17490  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
       "17491  20799                          What Keeps the F-35 Alive   \n",
       "\n",
       "                                          author  \\\n",
       "0                                  Darrell Lucus   \n",
       "1                                Daniel J. Flynn   \n",
       "2                             Consortiumnews.com   \n",
       "3                                Jessica Purkiss   \n",
       "4                                 Howard Portnoy   \n",
       "...                                          ...   \n",
       "17487                              Jerome Hudson   \n",
       "17488                           Benjamin Hoffman   \n",
       "17489  Michael J. de la Merced and Rachel Abrams   \n",
       "17490                                Alex Ansary   \n",
       "17491                              David Swanson   \n",
       "\n",
       "                                                    text  label  \\\n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
       "1      Ever get the feeling your life circles the rou...      0   \n",
       "2      Why the Truth Might Get You Fired October 29, ...      1   \n",
       "3      Videos 15 Civilians Killed In Single US Airstr...      1   \n",
       "4      Print \\nAn Iranian woman has been sentenced to...      1   \n",
       "...                                                  ...    ...   \n",
       "17487  Rapper T. I. unloaded on black celebrities who...      0   \n",
       "17488  When the Green Bay Packers lost to the Washing...      0   \n",
       "17489  The Macy’s of today grew from the union of sev...      0   \n",
       "17490  NATO, Russia To Hold Parallel Exercises In Bal...      1   \n",
       "17491    David Swanson is an author, activist, journa...      1   \n",
       "\n",
       "                                             text_tokens  dem_bias  neutral  \\\n",
       "0      [house, dem, aide, didnt, even, see, comeys, l...      0.49     0.02   \n",
       "1      [ever, get, feeling, life, circle, roundabout,...      0.52     0.01   \n",
       "2      [truth, might, get, fired, october, tension, i...      0.53     0.08   \n",
       "3      [video, civilian, killed, single, u, airstrike...      0.24     0.19   \n",
       "4      [print, iranian, woman, sentenced, six, year, ...      0.47     0.17   \n",
       "...                                                  ...       ...      ...   \n",
       "17487  [rapper, unloaded, black, celebrity, met, dona...      0.47     0.05   \n",
       "17488  [green, bay, packer, lost, washington, redskin...      0.23     0.22   \n",
       "17489  [macys, today, grew, union, several, great, na...      0.31     0.13   \n",
       "17490  [nato, russia, hold, parallel, exercise, balka...      0.28     0.25   \n",
       "17491  [david, swanson, author, activist, journalist,...      0.40     0.18   \n",
       "\n",
       "       rep_bias  \n",
       "0          0.49  \n",
       "1          0.47  \n",
       "2          0.39  \n",
       "3          0.57  \n",
       "4          0.36  \n",
       "...         ...  \n",
       "17487      0.48  \n",
       "17488      0.55  \n",
       "17489      0.56  \n",
       "17490      0.47  \n",
       "17491      0.42  \n",
       "\n",
       "[17492 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally, we'll rejoin the predictions to the original dataset.\n",
    "fn_kag_reduced = fn_kag_tok.copy().reset_index(drop=True)\n",
    "fn_kag_reduced = fn_kag_reduced.join(preds)\n",
    "fn_kag_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing on the Model part (copied from Jame's code)\n",
    "## Part 2: Clustering\n",
    "Once we have all the features we want, we'll do unsupervised clustering. Ideally we'd want to do some evaluations to find an ideal number of clusters, but for now we'll just go with 4.\n",
    "\n",
    "We'll need to re-vectorize the text, as the political bias vectors won't work here. Also, we'd probably want to vectorize both headline and article body, but for now I'll just vectorize the article body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From this point on, we should be concerned with data leakage. Everything prior to now could in theory be applied to live data. We'll go ahead and split the data out into train and test sets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(fn_kag_reduced.drop(columns=['label']), fn_kag_reduced.label, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we already have the tokenized text from above, we can just go ahead and train the new Word2Vec model on those tokens.\n",
    "wv_mod = Word2Vec(X_train['text_tokens'], seed = RANDOM_SEED)\n",
    "wv_mod.save(\"models/fn_w2v_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again we'll extract and average the word vectors.\n",
    "vectors = wv_mod.wv\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in X_train.text_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll join the new word vectors with the bias estimates we generate above.\n",
    "X_train_all = vec_frame.join(X_train).drop(columns=['id','title','author','text','text_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally we'll build our clustering model...\n",
    "cls = KMeans(4, random_state=RANDOM_SEED).fit(X_train_all)\n",
    "filename = \"models/cluster_mod.pkl\"\n",
    "pickle.dump(cls, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>dem_bias</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rep_bias</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.009658</td>\n",
       "      <td>0.005730</td>\n",
       "      <td>0.042834</td>\n",
       "      <td>0.046615</td>\n",
       "      <td>-0.047032</td>\n",
       "      <td>0.023168</td>\n",
       "      <td>0.037267</td>\n",
       "      <td>0.036255</td>\n",
       "      <td>-0.016245</td>\n",
       "      <td>-0.064646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019453</td>\n",
       "      <td>0.015911</td>\n",
       "      <td>-0.018045</td>\n",
       "      <td>0.006621</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>-0.032222</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.035299</td>\n",
       "      <td>-0.013338</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>-0.008684</td>\n",
       "      <td>-0.026056</td>\n",
       "      <td>-0.013706</td>\n",
       "      <td>0.007896</td>\n",
       "      <td>0.012592</td>\n",
       "      <td>-0.011828</td>\n",
       "      <td>-0.098827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022860</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.025123</td>\n",
       "      <td>0.037479</td>\n",
       "      <td>-0.004328</td>\n",
       "      <td>0.042627</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.008059</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.015817</td>\n",
       "      <td>-0.005587</td>\n",
       "      <td>-0.045881</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>-0.020504</td>\n",
       "      <td>-0.021675</td>\n",
       "      <td>-0.053884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011687</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>-0.013914</td>\n",
       "      <td>0.016798</td>\n",
       "      <td>0.011421</td>\n",
       "      <td>0.013591</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.013599</td>\n",
       "      <td>-0.020696</td>\n",
       "      <td>0.048159</td>\n",
       "      <td>0.041106</td>\n",
       "      <td>-0.050609</td>\n",
       "      <td>0.007193</td>\n",
       "      <td>0.032249</td>\n",
       "      <td>-0.023648</td>\n",
       "      <td>-0.026226</td>\n",
       "      <td>-0.074807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>0.024434</td>\n",
       "      <td>0.011079</td>\n",
       "      <td>0.023782</td>\n",
       "      <td>0.025636</td>\n",
       "      <td>0.011899</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010908</td>\n",
       "      <td>-0.023946</td>\n",
       "      <td>0.039867</td>\n",
       "      <td>0.039451</td>\n",
       "      <td>-0.071986</td>\n",
       "      <td>0.025116</td>\n",
       "      <td>0.054771</td>\n",
       "      <td>-0.024438</td>\n",
       "      <td>0.041491</td>\n",
       "      <td>-0.095228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033196</td>\n",
       "      <td>-0.005189</td>\n",
       "      <td>0.013563</td>\n",
       "      <td>0.037562</td>\n",
       "      <td>0.010634</td>\n",
       "      <td>-0.005792</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13988</th>\n",
       "      <td>-0.064868</td>\n",
       "      <td>-0.106135</td>\n",
       "      <td>-0.094492</td>\n",
       "      <td>-0.074123</td>\n",
       "      <td>-0.019983</td>\n",
       "      <td>0.037429</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>-0.220459</td>\n",
       "      <td>0.085949</td>\n",
       "      <td>0.172327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024090</td>\n",
       "      <td>-0.058535</td>\n",
       "      <td>-0.052388</td>\n",
       "      <td>-0.091403</td>\n",
       "      <td>-0.074858</td>\n",
       "      <td>-0.058445</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13989</th>\n",
       "      <td>0.009163</td>\n",
       "      <td>-0.029318</td>\n",
       "      <td>0.051790</td>\n",
       "      <td>0.034272</td>\n",
       "      <td>-0.045128</td>\n",
       "      <td>0.036544</td>\n",
       "      <td>-0.007169</td>\n",
       "      <td>0.028784</td>\n",
       "      <td>-0.014257</td>\n",
       "      <td>-0.076432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024975</td>\n",
       "      <td>0.016809</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>0.018483</td>\n",
       "      <td>-0.002483</td>\n",
       "      <td>-0.008261</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13990</th>\n",
       "      <td>0.010447</td>\n",
       "      <td>-0.019237</td>\n",
       "      <td>0.020061</td>\n",
       "      <td>0.006971</td>\n",
       "      <td>-0.033006</td>\n",
       "      <td>-0.022863</td>\n",
       "      <td>0.033802</td>\n",
       "      <td>0.031342</td>\n",
       "      <td>-0.021314</td>\n",
       "      <td>-0.098133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.007989</td>\n",
       "      <td>-0.009117</td>\n",
       "      <td>0.009067</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>-0.004169</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13991</th>\n",
       "      <td>-0.028158</td>\n",
       "      <td>-0.016735</td>\n",
       "      <td>0.028893</td>\n",
       "      <td>0.067358</td>\n",
       "      <td>-0.007451</td>\n",
       "      <td>0.041093</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>0.049816</td>\n",
       "      <td>-0.057361</td>\n",
       "      <td>-0.043732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037840</td>\n",
       "      <td>0.015241</td>\n",
       "      <td>0.006647</td>\n",
       "      <td>-0.001574</td>\n",
       "      <td>-0.044859</td>\n",
       "      <td>-0.015713</td>\n",
       "      <td>0.327333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.422667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13992</th>\n",
       "      <td>0.016288</td>\n",
       "      <td>-0.012518</td>\n",
       "      <td>0.014834</td>\n",
       "      <td>0.028610</td>\n",
       "      <td>-0.042497</td>\n",
       "      <td>0.029944</td>\n",
       "      <td>-0.020492</td>\n",
       "      <td>0.033255</td>\n",
       "      <td>-0.020810</td>\n",
       "      <td>-0.079006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023690</td>\n",
       "      <td>0.034915</td>\n",
       "      <td>0.018710</td>\n",
       "      <td>0.039326</td>\n",
       "      <td>-0.004540</td>\n",
       "      <td>0.005171</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13993 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -0.009658  0.005730  0.042834  0.046615 -0.047032  0.023168  0.037267   \n",
       "1      0.035299 -0.013338  0.016556 -0.008684 -0.026056 -0.013706  0.007896   \n",
       "2     -0.008059  0.002088  0.008326  0.015817 -0.005587 -0.045881  0.029630   \n",
       "3     -0.013599 -0.020696  0.048159  0.041106 -0.050609  0.007193  0.032249   \n",
       "4      0.010908 -0.023946  0.039867  0.039451 -0.071986  0.025116  0.054771   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "13988 -0.064868 -0.106135 -0.094492 -0.074123 -0.019983  0.037429  0.000613   \n",
       "13989  0.009163 -0.029318  0.051790  0.034272 -0.045128  0.036544 -0.007169   \n",
       "13990  0.010447 -0.019237  0.020061  0.006971 -0.033006 -0.022863  0.033802   \n",
       "13991 -0.028158 -0.016735  0.028893  0.067358 -0.007451  0.041093  0.004991   \n",
       "13992  0.016288 -0.012518  0.014834  0.028610 -0.042497  0.029944 -0.020492   \n",
       "\n",
       "              7         8         9  ...        94        95        96  \\\n",
       "0      0.036255 -0.016245 -0.064646  ...  0.019453  0.015911 -0.018045   \n",
       "1      0.012592 -0.011828 -0.098827  ...  0.022860  0.019518  0.025123   \n",
       "2     -0.020504 -0.021675 -0.053884  ...  0.011687  0.039100 -0.013914   \n",
       "3     -0.023648 -0.026226 -0.074807  ...  0.010859  0.024434  0.011079   \n",
       "4     -0.024438  0.041491 -0.095228  ...  0.033196 -0.005189  0.013563   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "13988 -0.220459  0.085949  0.172327  ...  0.024090 -0.058535 -0.052388   \n",
       "13989  0.028784 -0.014257 -0.076432  ...  0.024975  0.016809  0.008687   \n",
       "13990  0.031342 -0.021314 -0.098133  ...  0.001802  0.007989 -0.009117   \n",
       "13991  0.049816 -0.057361 -0.043732  ...  0.037840  0.015241  0.006647   \n",
       "13992  0.033255 -0.020810 -0.079006  ...  0.023690  0.034915  0.018710   \n",
       "\n",
       "             97        98        99  dem_bias  neutral  rep_bias  cluster  \n",
       "0      0.006621  0.003714 -0.032222  0.340000     0.12  0.540000        0  \n",
       "1      0.037479 -0.004328  0.042627  0.410000     0.14  0.450000        2  \n",
       "2      0.016798  0.011421  0.013591  0.330000     0.12  0.550000        0  \n",
       "3      0.023782  0.025636  0.011899  0.470000     0.11  0.420000        2  \n",
       "4      0.037562  0.010634 -0.005792  0.348000     0.24  0.412000        1  \n",
       "...         ...       ...       ...       ...      ...       ...      ...  \n",
       "13988 -0.091403 -0.074858 -0.058445  0.237500     0.36  0.402500        3  \n",
       "13989  0.018483 -0.002483 -0.008261  0.420000     0.16  0.420000        2  \n",
       "13990  0.009067  0.001602 -0.004169  0.420000     0.17  0.410000        1  \n",
       "13991 -0.001574 -0.044859 -0.015713  0.327333     0.25  0.422667        1  \n",
       "13992  0.039326 -0.004540  0.005171  0.440000     0.07  0.490000        2  \n",
       "\n",
       "[13993 rows x 104 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#...and add the predicted clusters back into the vector dataframe.\n",
    "X_train_all['cluster'] = cls.predict(X_train_all)\n",
    "X_train_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Supervised Learning\n",
    "\n",
    "Now that we have all of our features and clusters, and article body text is already vectorized, we can train a classifier to predict whether a given article is misinformation or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>dem_bias</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rep_bias</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.004069</td>\n",
       "      <td>0.006721</td>\n",
       "      <td>0.025754</td>\n",
       "      <td>0.016396</td>\n",
       "      <td>-0.024229</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>-0.005376</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>-0.014789</td>\n",
       "      <td>-0.061167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.015933</td>\n",
       "      <td>0.034293</td>\n",
       "      <td>0.010748</td>\n",
       "      <td>-0.005926</td>\n",
       "      <td>0.024244</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007615</td>\n",
       "      <td>0.021091</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>0.035196</td>\n",
       "      <td>-0.026442</td>\n",
       "      <td>0.019393</td>\n",
       "      <td>0.025404</td>\n",
       "      <td>0.076108</td>\n",
       "      <td>-0.016611</td>\n",
       "      <td>-0.025038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039519</td>\n",
       "      <td>0.040006</td>\n",
       "      <td>0.030866</td>\n",
       "      <td>0.032353</td>\n",
       "      <td>-0.008771</td>\n",
       "      <td>-0.000636</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.015921</td>\n",
       "      <td>-0.017781</td>\n",
       "      <td>0.027126</td>\n",
       "      <td>0.039706</td>\n",
       "      <td>-0.072932</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>0.049133</td>\n",
       "      <td>-0.028249</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>-0.090377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.026980</td>\n",
       "      <td>-0.000872</td>\n",
       "      <td>0.036489</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>-0.001185</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026510</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>-0.015315</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>-0.045030</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>-0.010983</td>\n",
       "      <td>-0.036082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050313</td>\n",
       "      <td>0.056651</td>\n",
       "      <td>-0.007929</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>-0.000486</td>\n",
       "      <td>0.025396</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.030441</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>0.020871</td>\n",
       "      <td>0.011121</td>\n",
       "      <td>-0.068780</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>0.026041</td>\n",
       "      <td>0.025270</td>\n",
       "      <td>0.028207</td>\n",
       "      <td>-0.053540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010415</td>\n",
       "      <td>0.031614</td>\n",
       "      <td>0.007730</td>\n",
       "      <td>0.036058</td>\n",
       "      <td>0.023472</td>\n",
       "      <td>0.025822</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3494</th>\n",
       "      <td>0.000622</td>\n",
       "      <td>-0.015777</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>0.022281</td>\n",
       "      <td>-0.019934</td>\n",
       "      <td>0.022051</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.027889</td>\n",
       "      <td>-0.018690</td>\n",
       "      <td>-0.037581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043006</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>0.006654</td>\n",
       "      <td>0.009844</td>\n",
       "      <td>-0.018182</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>-0.003556</td>\n",
       "      <td>-0.001852</td>\n",
       "      <td>0.027409</td>\n",
       "      <td>0.018975</td>\n",
       "      <td>-0.044493</td>\n",
       "      <td>-0.012646</td>\n",
       "      <td>-0.015225</td>\n",
       "      <td>-0.051780</td>\n",
       "      <td>-0.019676</td>\n",
       "      <td>-0.039484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017797</td>\n",
       "      <td>0.033542</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>-0.028908</td>\n",
       "      <td>-0.022689</td>\n",
       "      <td>-0.023097</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>-0.005016</td>\n",
       "      <td>-0.026904</td>\n",
       "      <td>-0.028986</td>\n",
       "      <td>-0.009276</td>\n",
       "      <td>-0.035803</td>\n",
       "      <td>-0.053517</td>\n",
       "      <td>0.012711</td>\n",
       "      <td>-0.000715</td>\n",
       "      <td>-0.050154</td>\n",
       "      <td>-0.062212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025777</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>-0.005290</td>\n",
       "      <td>0.012638</td>\n",
       "      <td>-0.013910</td>\n",
       "      <td>-0.047990</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>-0.003582</td>\n",
       "      <td>-0.017635</td>\n",
       "      <td>0.022182</td>\n",
       "      <td>0.072596</td>\n",
       "      <td>-0.027446</td>\n",
       "      <td>0.024533</td>\n",
       "      <td>-0.008718</td>\n",
       "      <td>0.059137</td>\n",
       "      <td>-0.027213</td>\n",
       "      <td>-0.041577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051082</td>\n",
       "      <td>0.046678</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.037541</td>\n",
       "      <td>-0.034289</td>\n",
       "      <td>-0.009565</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>-0.061865</td>\n",
       "      <td>-0.025476</td>\n",
       "      <td>0.023372</td>\n",
       "      <td>0.063323</td>\n",
       "      <td>-0.034211</td>\n",
       "      <td>0.012190</td>\n",
       "      <td>-0.025201</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.006571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031042</td>\n",
       "      <td>0.040054</td>\n",
       "      <td>-0.017295</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>-0.016122</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3499 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.004069  0.006721  0.025754  0.016396 -0.024229  0.014822 -0.005376   \n",
       "1     0.007615  0.021091  0.009161  0.035196 -0.026442  0.019393  0.025404   \n",
       "2    -0.015921 -0.017781  0.027126  0.039706 -0.072932 -0.000181  0.049133   \n",
       "3     0.026510  0.011171 -0.015315  0.002247  0.002525  0.007189 -0.045030   \n",
       "4     0.030441  0.015128  0.020871  0.011121 -0.068780  0.009686  0.026041   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3494  0.000622 -0.015777  0.001780  0.022281 -0.019934  0.022051  0.005040   \n",
       "3495 -0.003556 -0.001852  0.027409  0.018975 -0.044493 -0.012646 -0.015225   \n",
       "3496 -0.005016 -0.026904 -0.028986 -0.009276 -0.035803 -0.053517  0.012711   \n",
       "3497 -0.003582 -0.017635  0.022182  0.072596 -0.027446  0.024533 -0.008718   \n",
       "3498 -0.061865 -0.025476  0.023372  0.063323 -0.034211  0.012190 -0.025201   \n",
       "\n",
       "             7         8         9  ...        94        95        96  \\\n",
       "0     0.004720 -0.014789 -0.061167  ...  0.000795  0.015933  0.034293   \n",
       "1     0.076108 -0.016611 -0.025038  ...  0.039519  0.040006  0.030866   \n",
       "2    -0.028249  0.002583 -0.090377  ...  0.006657  0.026980 -0.000872   \n",
       "3     0.004801 -0.010983 -0.036082  ...  0.050313  0.056651 -0.007929   \n",
       "4     0.025270  0.028207 -0.053540  ...  0.010415  0.031614  0.007730   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3494  0.027889 -0.018690 -0.037581  ...  0.043006  0.008060  0.006654   \n",
       "3495 -0.051780 -0.019676 -0.039484  ...  0.017797  0.033542  0.000850   \n",
       "3496 -0.000715 -0.050154 -0.062212  ...  0.025777 -0.008230 -0.005290   \n",
       "3497  0.059137 -0.027213 -0.041577  ...  0.051082  0.046678  0.002067   \n",
       "3498  0.016879  0.003855  0.006571  ...  0.031042  0.040054 -0.017295   \n",
       "\n",
       "            97        98        99  dem_bias  neutral  rep_bias  cluster  \n",
       "0     0.010748 -0.005926  0.024244      0.54     0.08      0.38        2  \n",
       "1     0.032353 -0.008771 -0.000636      0.59     0.02      0.39        2  \n",
       "2     0.036489  0.011354 -0.001185      0.29     0.30      0.41        1  \n",
       "3     0.030120 -0.000486  0.025396      0.32     0.18      0.50        1  \n",
       "4     0.036058  0.023472  0.025822      0.48     0.07      0.45        2  \n",
       "...        ...       ...       ...       ...      ...       ...      ...  \n",
       "3494  0.009844 -0.018182  0.003670      0.54     0.06      0.40        2  \n",
       "3495 -0.028908 -0.022689 -0.023097      0.12     0.28      0.60        1  \n",
       "3496  0.012638 -0.013910 -0.047990      0.13     0.24      0.63        1  \n",
       "3497  0.037541 -0.034289 -0.009565      0.56     0.06      0.38        2  \n",
       "3498  0.000189 -0.016122 -0.032334      0.12     0.40      0.48        1  \n",
       "\n",
       "[3499 rows x 104 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to apply the vectorization and clustering from above to the test data.\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in X_test.text_tokens])\n",
    "X_test_all = vec_frame.join(X_test).drop(columns=['id','title','author','text','text_tokens'])\n",
    "X_test_all['cluster'] = cls.predict(X_test_all)\n",
    "X_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8939697056301801"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "clf.fit(X_train_all, y_train)\n",
    "filename = \"models/fn_classifier_model.pkl\"\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "clf.score(X_test_all, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Export CSV\n",
    "\n",
    "So now we have 2 datasets (3 if you count the unused 'test' set from the Kaggle fake news data). We want to have 1 large dataset to power the dashboard. Let's load each dataset into a dataframe, vectorize the text, predict a cluster, and predict whether it's misinformation or not. We'll start fresh to keep things simple, and we'll just use article body text.\n",
    "\n",
    "This part can really stand on its own... should probably pull it out into a separate notebook/script? Only issue is we'd have to repeat the code for the NLP steps, unless we save them into their own script as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load each datafile we want to process\n",
    "data_bias=pd.read_excel('assets/pb_spinde.xlsx')\n",
    "data_news_1=pd.read_csv('assets/fn_kagg_train.csv')\n",
    "data_news_2=pd.read_csv('assets/fn_kagg_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all of our models\n",
    "pb_vec_mod = Word2Vec.load('models/pb_w2v_model.pkl')\n",
    "fn_vec_mod = Word2Vec.load('models/fn_w2v_model.pkl')\n",
    "cls = pickle.load(open('models/cluster_mod.pkl', 'rb'))\n",
    "pb_clf = pickle.load(open('models/pb_classifier_model.pkl', 'rb'))\n",
    "fn_clf = pickle.load(open('models/fn_classifier_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27549</th>\n",
       "      <td>Of all the dysfunctions that plague the world’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27550</th>\n",
       "      <td>WASHINGTON  —   Gov. John Kasich of Ohio on Tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27551</th>\n",
       "      <td>Good morning. (Want to get California Today by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27552</th>\n",
       "      <td>« Previous - Next » 300 US Marines To Be Deplo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27553</th>\n",
       "      <td>Perhaps you’ve seen the new TV series whose pi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27554 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               full_text\n",
       "0      YouTube says no ‘deepfakes’ or ‘birther’ video...\n",
       "1      FRISCO, Texas — The increasingly bitter disput...\n",
       "2      Speaking to the country for the first time fro...\n",
       "3      A professor who teaches climate change classes...\n",
       "4      The left has a thing for taking babies hostage...\n",
       "...                                                  ...\n",
       "27549  Of all the dysfunctions that plague the world’...\n",
       "27550  WASHINGTON  —   Gov. John Kasich of Ohio on Tu...\n",
       "27551  Good morning. (Want to get California Today by...\n",
       "27552  « Previous - Next » 300 US Marines To Be Deplo...\n",
       "27553  Perhaps you’ve seen the new TV series whose pi...\n",
       "\n",
       "[27554 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grab just the article text, drop empty cells, and stack the dataframes.\n",
    "db_text = data_bias.article\n",
    "dn1_text = data_news_1.text\n",
    "dn2_text = data_news_2.text\n",
    "full_data = pd.DataFrame(pd.concat([db_text, dn1_text, dn2_text], axis = 0).dropna().reset_index(drop=True))\n",
    "full_data.columns = ['full_text']\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>[youtube, says, deepfakes, birther, videos, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "      <td>[frisco, texas, increasingly, bitter, dispute,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>[speaking, country, first, time, oval, office,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>[professor, teaches, climate, change, classes,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>[left, thing, taking, babies, hostage, perfect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25834</th>\n",
       "      <td>Of all the dysfunctions that plague the world’...</td>\n",
       "      <td>[dysfunctions, plague, worlds, megacities, non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25835</th>\n",
       "      <td>WASHINGTON  —   Gov. John Kasich of Ohio on Tu...</td>\n",
       "      <td>[washington, gov, john, kasich, ohio, tuesday,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25836</th>\n",
       "      <td>Good morning. (Want to get California Today by...</td>\n",
       "      <td>[good, morning, want, get, california, today, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25837</th>\n",
       "      <td>« Previous - Next » 300 US Marines To Be Deplo...</td>\n",
       "      <td>[previous, next, us, marines, deployed, russia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25838</th>\n",
       "      <td>Perhaps you’ve seen the new TV series whose pi...</td>\n",
       "      <td>[perhaps, youve, seen, new, tv, series, whose,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25839 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               full_text  \\\n",
       "0      YouTube says no ‘deepfakes’ or ‘birther’ video...   \n",
       "1      FRISCO, Texas — The increasingly bitter disput...   \n",
       "2      Speaking to the country for the first time fro...   \n",
       "3      A professor who teaches climate change classes...   \n",
       "4      The left has a thing for taking babies hostage...   \n",
       "...                                                  ...   \n",
       "25834  Of all the dysfunctions that plague the world’...   \n",
       "25835  WASHINGTON  —   Gov. John Kasich of Ohio on Tu...   \n",
       "25836  Good morning. (Want to get California Today by...   \n",
       "25837  « Previous - Next » 300 US Marines To Be Deplo...   \n",
       "25838  Perhaps you’ve seen the new TV series whose pi...   \n",
       "\n",
       "                                                  tokens  \n",
       "0      [youtube, says, deepfakes, birther, videos, to...  \n",
       "1      [frisco, texas, increasingly, bitter, dispute,...  \n",
       "2      [speaking, country, first, time, oval, office,...  \n",
       "3      [professor, teaches, climate, change, classes,...  \n",
       "4      [left, thing, taking, babies, hostage, perfect...  \n",
       "...                                                  ...  \n",
       "25834  [dysfunctions, plague, worlds, megacities, non...  \n",
       "25835  [washington, gov, john, kasich, ohio, tuesday,...  \n",
       "25836  [good, morning, want, get, california, today, ...  \n",
       "25837  [previous, next, us, marines, deployed, russia...  \n",
       "25838  [perhaps, youve, seen, new, tv, series, whose,...  \n",
       "\n",
       "[25839 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we'll apply all the NLP steps from above to the full series. This can take a while. I'm skipping the lemmatization and stemming for now...\n",
    "# full_data_clean = full_data.copy()\n",
    "# full_data_clean['processing'] = full_data_clean.full_text.apply(lambda x: x.encode('ascii', 'ignore').decode(\"ascii\",\"ignore\"))\n",
    "# full_data_clean['processing'] = full_data_clean.processing.apply(round1)\n",
    "# full_data_clean['processing'] = full_data_clean.processing.apply(round2)\n",
    "# full_data_clean['processing'] = full_data_clean.processing.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "# full_data_clean['tokens'] = full_data_clean.processing.apply(lambda y: [x for x in word_tokenize(y)])\n",
    "\n",
    "# full_data_clean['tmp'] = full_data_clean['tokens'].apply(lambda x: len(x))\n",
    "# full_data_clean = full_data_clean[full_data_clean['tmp']>30]\n",
    "# full_data_clean = full_data_clean.drop(columns=['tmp','processing'])\n",
    "\n",
    "# full_data_clean.to_pickle('clean_full_data.pkl')\n",
    "full_data_clean = pd.read_pickle('clean_full_data.pkl').reset_index(drop=True)\n",
    "full_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>dem_bias</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rep_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>[youtube, says, deepfakes, birther, videos, to...</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "      <td>[frisco, texas, increasingly, bitter, dispute,...</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>[speaking, country, first, time, oval, office,...</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>[professor, teaches, climate, change, classes,...</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>[left, thing, taking, babies, hostage, perfect...</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25834</th>\n",
       "      <td>Of all the dysfunctions that plague the world’...</td>\n",
       "      <td>[dysfunctions, plague, worlds, megacities, non...</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25835</th>\n",
       "      <td>WASHINGTON  —   Gov. John Kasich of Ohio on Tu...</td>\n",
       "      <td>[washington, gov, john, kasich, ohio, tuesday,...</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25836</th>\n",
       "      <td>Good morning. (Want to get California Today by...</td>\n",
       "      <td>[good, morning, want, get, california, today, ...</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25837</th>\n",
       "      <td>« Previous - Next » 300 US Marines To Be Deplo...</td>\n",
       "      <td>[previous, next, us, marines, deployed, russia...</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.623077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25838</th>\n",
       "      <td>Perhaps you’ve seen the new TV series whose pi...</td>\n",
       "      <td>[perhaps, youve, seen, new, tv, series, whose,...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25839 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               full_text  \\\n",
       "0      YouTube says no ‘deepfakes’ or ‘birther’ video...   \n",
       "1      FRISCO, Texas — The increasingly bitter disput...   \n",
       "2      Speaking to the country for the first time fro...   \n",
       "3      A professor who teaches climate change classes...   \n",
       "4      The left has a thing for taking babies hostage...   \n",
       "...                                                  ...   \n",
       "25834  Of all the dysfunctions that plague the world’...   \n",
       "25835  WASHINGTON  —   Gov. John Kasich of Ohio on Tu...   \n",
       "25836  Good morning. (Want to get California Today by...   \n",
       "25837  « Previous - Next » 300 US Marines To Be Deplo...   \n",
       "25838  Perhaps you’ve seen the new TV series whose pi...   \n",
       "\n",
       "                                                  tokens  dem_bias  neutral  \\\n",
       "0      [youtube, says, deepfakes, birther, videos, to...  0.350000     0.33   \n",
       "1      [frisco, texas, increasingly, bitter, dispute,...  0.490000     0.12   \n",
       "2      [speaking, country, first, time, oval, office,...  0.730000     0.08   \n",
       "3      [professor, teaches, climate, change, classes,...  0.520000     0.02   \n",
       "4      [left, thing, taking, babies, hostage, perfect...  0.140000     0.01   \n",
       "...                                                  ...       ...      ...   \n",
       "25834  [dysfunctions, plague, worlds, megacities, non...  0.310000     0.06   \n",
       "25835  [washington, gov, john, kasich, ohio, tuesday,...  0.320000     0.38   \n",
       "25836  [good, morning, want, get, california, today, ...  0.190000     0.18   \n",
       "25837  [previous, next, us, marines, deployed, russia...  0.326923     0.05   \n",
       "25838  [perhaps, youve, seen, new, tv, series, whose,...  0.160000     0.01   \n",
       "\n",
       "       rep_bias  \n",
       "0      0.320000  \n",
       "1      0.390000  \n",
       "2      0.190000  \n",
       "3      0.460000  \n",
       "4      0.850000  \n",
       "...         ...  \n",
       "25834  0.630000  \n",
       "25835  0.300000  \n",
       "25836  0.630000  \n",
       "25837  0.623077  \n",
       "25838  0.830000  \n",
       "\n",
       "[25839 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we'll use the pb vectorizer and generate political bias predictions for the whole dataset.\n",
    "vectors = pb_vec_mod.wv\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in full_data_clean.tokens])\n",
    "preds = pd.DataFrame(pb_clf.predict_proba(vec_frame), columns=['dem_bias','neutral','rep_bias'])\n",
    "full_data_clean = full_data_clean.join(preds)\n",
    "full_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>dem_bias</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rep_bias</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>[youtube, says, deepfakes, birther, videos, to...</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "      <td>[frisco, texas, increasingly, bitter, dispute,...</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>[speaking, country, first, time, oval, office,...</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>[professor, teaches, climate, change, classes,...</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>[left, thing, taking, babies, hostage, perfect...</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25834</th>\n",
       "      <td>Of all the dysfunctions that plague the world’...</td>\n",
       "      <td>[dysfunctions, plague, worlds, megacities, non...</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25835</th>\n",
       "      <td>WASHINGTON  —   Gov. John Kasich of Ohio on Tu...</td>\n",
       "      <td>[washington, gov, john, kasich, ohio, tuesday,...</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25836</th>\n",
       "      <td>Good morning. (Want to get California Today by...</td>\n",
       "      <td>[good, morning, want, get, california, today, ...</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25837</th>\n",
       "      <td>« Previous - Next » 300 US Marines To Be Deplo...</td>\n",
       "      <td>[previous, next, us, marines, deployed, russia...</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.623077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25838</th>\n",
       "      <td>Perhaps you’ve seen the new TV series whose pi...</td>\n",
       "      <td>[perhaps, youve, seen, new, tv, series, whose,...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25839 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               full_text  \\\n",
       "0      YouTube says no ‘deepfakes’ or ‘birther’ video...   \n",
       "1      FRISCO, Texas — The increasingly bitter disput...   \n",
       "2      Speaking to the country for the first time fro...   \n",
       "3      A professor who teaches climate change classes...   \n",
       "4      The left has a thing for taking babies hostage...   \n",
       "...                                                  ...   \n",
       "25834  Of all the dysfunctions that plague the world’...   \n",
       "25835  WASHINGTON  —   Gov. John Kasich of Ohio on Tu...   \n",
       "25836  Good morning. (Want to get California Today by...   \n",
       "25837  « Previous - Next » 300 US Marines To Be Deplo...   \n",
       "25838  Perhaps you’ve seen the new TV series whose pi...   \n",
       "\n",
       "                                                  tokens  dem_bias  neutral  \\\n",
       "0      [youtube, says, deepfakes, birther, videos, to...  0.350000     0.33   \n",
       "1      [frisco, texas, increasingly, bitter, dispute,...  0.490000     0.12   \n",
       "2      [speaking, country, first, time, oval, office,...  0.730000     0.08   \n",
       "3      [professor, teaches, climate, change, classes,...  0.520000     0.02   \n",
       "4      [left, thing, taking, babies, hostage, perfect...  0.140000     0.01   \n",
       "...                                                  ...       ...      ...   \n",
       "25834  [dysfunctions, plague, worlds, megacities, non...  0.310000     0.06   \n",
       "25835  [washington, gov, john, kasich, ohio, tuesday,...  0.320000     0.38   \n",
       "25836  [good, morning, want, get, california, today, ...  0.190000     0.18   \n",
       "25837  [previous, next, us, marines, deployed, russia...  0.326923     0.05   \n",
       "25838  [perhaps, youve, seen, new, tv, series, whose,...  0.160000     0.01   \n",
       "\n",
       "       rep_bias  cluster  \n",
       "0      0.320000        1  \n",
       "1      0.390000        2  \n",
       "2      0.190000        2  \n",
       "3      0.460000        2  \n",
       "4      0.850000        0  \n",
       "...         ...      ...  \n",
       "25834  0.630000        0  \n",
       "25835  0.300000        1  \n",
       "25836  0.630000        0  \n",
       "25837  0.623077        0  \n",
       "25838  0.830000        0  \n",
       "\n",
       "[25839 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we'll predict clusters using the fake news w2v model.\n",
    "vectors = fn_vec_mod.wv\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in full_data_clean.tokens])\n",
    "cluster_frame = vec_frame.join(full_data_clean).drop(columns=['full_text','tokens'])\n",
    "full_data_clean['cluster'] = cls.predict(cluster_frame)\n",
    "full_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>dem_bias</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rep_bias</th>\n",
       "      <th>cluster</th>\n",
       "      <th>not_misinfo</th>\n",
       "      <th>misinfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>[youtube, says, deepfakes, birther, videos, to...</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "      <td>[frisco, texas, increasingly, bitter, dispute,...</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>[speaking, country, first, time, oval, office,...</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>[professor, teaches, climate, change, classes,...</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>[left, thing, taking, babies, hostage, perfect...</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25834</th>\n",
       "      <td>Of all the dysfunctions that plague the world’...</td>\n",
       "      <td>[dysfunctions, plague, worlds, megacities, non...</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25835</th>\n",
       "      <td>WASHINGTON  —   Gov. John Kasich of Ohio on Tu...</td>\n",
       "      <td>[washington, gov, john, kasich, ohio, tuesday,...</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25836</th>\n",
       "      <td>Good morning. (Want to get California Today by...</td>\n",
       "      <td>[good, morning, want, get, california, today, ...</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25837</th>\n",
       "      <td>« Previous - Next » 300 US Marines To Be Deplo...</td>\n",
       "      <td>[previous, next, us, marines, deployed, russia...</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.623077</td>\n",
       "      <td>0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25838</th>\n",
       "      <td>Perhaps you’ve seen the new TV series whose pi...</td>\n",
       "      <td>[perhaps, youve, seen, new, tv, series, whose,...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25839 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               full_text  \\\n",
       "0      YouTube says no ‘deepfakes’ or ‘birther’ video...   \n",
       "1      FRISCO, Texas — The increasingly bitter disput...   \n",
       "2      Speaking to the country for the first time fro...   \n",
       "3      A professor who teaches climate change classes...   \n",
       "4      The left has a thing for taking babies hostage...   \n",
       "...                                                  ...   \n",
       "25834  Of all the dysfunctions that plague the world’...   \n",
       "25835  WASHINGTON  —   Gov. John Kasich of Ohio on Tu...   \n",
       "25836  Good morning. (Want to get California Today by...   \n",
       "25837  « Previous - Next » 300 US Marines To Be Deplo...   \n",
       "25838  Perhaps you’ve seen the new TV series whose pi...   \n",
       "\n",
       "                                                  tokens  dem_bias  neutral  \\\n",
       "0      [youtube, says, deepfakes, birther, videos, to...  0.350000     0.33   \n",
       "1      [frisco, texas, increasingly, bitter, dispute,...  0.490000     0.12   \n",
       "2      [speaking, country, first, time, oval, office,...  0.730000     0.08   \n",
       "3      [professor, teaches, climate, change, classes,...  0.520000     0.02   \n",
       "4      [left, thing, taking, babies, hostage, perfect...  0.140000     0.01   \n",
       "...                                                  ...       ...      ...   \n",
       "25834  [dysfunctions, plague, worlds, megacities, non...  0.310000     0.06   \n",
       "25835  [washington, gov, john, kasich, ohio, tuesday,...  0.320000     0.38   \n",
       "25836  [good, morning, want, get, california, today, ...  0.190000     0.18   \n",
       "25837  [previous, next, us, marines, deployed, russia...  0.326923     0.05   \n",
       "25838  [perhaps, youve, seen, new, tv, series, whose,...  0.160000     0.01   \n",
       "\n",
       "       rep_bias  cluster  not_misinfo  misinfo  \n",
       "0      0.320000        1         0.55     0.45  \n",
       "1      0.390000        2         0.98     0.02  \n",
       "2      0.190000        2         0.83     0.17  \n",
       "3      0.460000        2         0.48     0.52  \n",
       "4      0.850000        0         0.27     0.73  \n",
       "...         ...      ...          ...      ...  \n",
       "25834  0.630000        0         0.75     0.25  \n",
       "25835  0.300000        1         0.93     0.07  \n",
       "25836  0.630000        0         0.93     0.07  \n",
       "25837  0.623077        0         0.16     0.84  \n",
       "25838  0.830000        0         0.86     0.14  \n",
       "\n",
       "[25839 rows x 8 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally, we'll generate predicted fake news probabilities using our fn classifier.\n",
    "class_frame = vec_frame.join(full_data_clean).drop(columns=['full_text','tokens'])\n",
    "preds = pd.DataFrame(fn_clf.predict_proba(class_frame), columns=['not_misinfo', 'misinfo'])\n",
    "full_data_clean = full_data_clean.join(preds)\n",
    "full_data_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_clean.to_csv(\"assets/all_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do:\n",
    "1. Put all NLP stuff in a function, so that we can just pass a column of text and get the clean tokens out.\n",
    "2. (Maybe?) Clean the notebook up into two scripts: one that builds and pickles the models, and one that takes a csv (or set of csvs/excels/whatever), runs all the models, and outputs a csv with all the data needed for the dashboard.\n",
    "3. ??Include headline text in models\n",
    "4. Do some deeper evaluations on the clustering and supervised learning portions (supervised portion is not critical, we have good accuracy right now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
