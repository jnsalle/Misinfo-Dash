{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install gensim\n",
    "# !pip install nltk\n",
    "# !pip install sklearn\n",
    "# !pip install numpy\n",
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With our text data, we are going to apply some of the text pre-processing techniques. Since this cleaning process can go on forever. There's always an exception to every cleaning steps. So, we're going to do this process in a few rounds.\n",
    "\n",
    "**Below are the steps we will be applying to our dataset:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "* Stemming & Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def clean_up(df):\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "        text = re.sub('\\w*\\d\\w*', '', text)\n",
    "        text = re.sub('[‘’“”—…]', '', text)\n",
    "        text = re.sub('\\n', '', text)\n",
    "        return text\n",
    "\n",
    "    def lemmatize_text(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "    data_clean = pd.DataFrame(df.apply(lambda x: clean_text(x)))\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    data_clean = data_clean.iloc[:,0].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "    data_clean = data_clean.apply(lambda x: x.encode('ascii', 'ignore').decode(\"ascii\",\"ignore\"))\n",
    "    data_clean = data_clean.apply(lambda y: [x for x in word_tokenize(y)])\n",
    "    data_clean = data_clean.apply(lemmatize_text)\n",
    "\n",
    "\n",
    "    data_clean = pd.DataFrame(data_clean)\n",
    "\n",
    "    return data_clean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaning data from DeepBlue for Bias Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_bias=pd.read_excel('assets/pb_spinde.xlsx')\n",
    "data_bias=data_bias[['article','type']]\n",
    "data_bias = data_bias.dropna().reset_index(drop=True)\n",
    "data_bias['type'] = data_bias.type.replace({'center':0,'left':-1,'right':1})\n",
    "data_bias_cleanned = clean_up(data_bias['article'])\n",
    "data_bias['tokens'] = data_bias_cleanned['article']\n",
    "data_bias"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1a: Political Bias Modeling\n",
    "\n",
    "First we want to build a model of political bias using features that will be available in our primary dataset. We'll import the Spinde political bias dataset and select the article text and bias rating columns. Then, we'll vectorize the article text and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                article  type  \\\n0     YouTube says no ‘deepfakes’ or ‘birther’ video...     0   \n1     FRISCO, Texas — The increasingly bitter disput...    -1   \n2     Speaking to the country for the first time fro...    -1   \n3     A professor who teaches climate change classes...     1   \n4     The left has a thing for taking babies hostage...     1   \n...                                                 ...   ...   \n1595  The House Democrats’ coronavirus recovery bill...     1   \n1596  There are many reasons that Republicans and co...    -1   \n1597  A man’s penis becomes a female penis once a ma...     1   \n1598  As a self-described Democratic socialist, Sen....     1   \n1599  CBS Late Show host Stephen Colbert claimed on ...     1   \n\n                                                 tokens  \n0     [youtube, say, deepfakes, birther, video, toug...  \n1     [frisco, texas, increasingly, bitter, dispute,...  \n2     [speaking, country, first, time, oval, office,...  \n3     [professor, teach, climate, change, class, sub...  \n4     [left, thing, taking, baby, hostage, perfect, ...  \n...                                                 ...  \n1595  [house, democrat, coronavirus, recovery, bill,...  \n1596  [many, reason, republican, conservative, activ...  \n1597  [man, penis, becomes, female, penis, man, decl...  \n1598  [selfdescribed, democratic, socialist, sen, be...  \n1599  [cbs, late, show, host, stephen, colbert, clai...  \n\n[1600 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article</th>\n      <th>type</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n      <td>0</td>\n      <td>[youtube, say, deepfakes, birther, video, toug...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n      <td>-1</td>\n      <td>[frisco, texas, increasingly, bitter, dispute,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Speaking to the country for the first time fro...</td>\n      <td>-1</td>\n      <td>[speaking, country, first, time, oval, office,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A professor who teaches climate change classes...</td>\n      <td>1</td>\n      <td>[professor, teach, climate, change, class, sub...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The left has a thing for taking babies hostage...</td>\n      <td>1</td>\n      <td>[left, thing, taking, baby, hostage, perfect, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1595</th>\n      <td>The House Democrats’ coronavirus recovery bill...</td>\n      <td>1</td>\n      <td>[house, democrat, coronavirus, recovery, bill,...</td>\n    </tr>\n    <tr>\n      <th>1596</th>\n      <td>There are many reasons that Republicans and co...</td>\n      <td>-1</td>\n      <td>[many, reason, republican, conservative, activ...</td>\n    </tr>\n    <tr>\n      <th>1597</th>\n      <td>A man’s penis becomes a female penis once a ma...</td>\n      <td>1</td>\n      <td>[man, penis, becomes, female, penis, man, decl...</td>\n    </tr>\n    <tr>\n      <th>1598</th>\n      <td>As a self-described Democratic socialist, Sen....</td>\n      <td>1</td>\n      <td>[selfdescribed, democratic, socialist, sen, be...</td>\n    </tr>\n    <tr>\n      <th>1599</th>\n      <td>CBS Late Show host Stephen Colbert claimed on ...</td>\n      <td>1</td>\n      <td>[cbs, late, show, host, stephen, colbert, clai...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1600 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb_reduced = data_bias.copy()\n",
    "pb_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Now we'll train the Word2Vec model on our text tokens.\n",
    "wv_mod = Word2Vec(pb_reduced['tokens'], seed = RANDOM_SEED)\n",
    "wv_mod.save(\"models/pb_classifier_model.pkl\") # models/pb_classifier_model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#We'll extract the vectors from the model...\n",
    "vectors = wv_mod.wv\n",
    "#...and since each word is a vector of 100 numbers, we'll take the mean of all word vectors in a given article \n",
    "#to represent the article as a whole\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in pb_reduced.tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Finally, we'll train a Random Forest classifier on the vectorized text to predict article bias.\n",
    "X_train, X_test, y_train, y_test = train_test_split(vec_frame, pb_reduced.type, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.79375"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "clf.fit(X_train, y_train)\n",
    "filename = \"models/pb_classifier_model.pkl\" #models/pb_classifier_model.pkl\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cleaning Data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data_news=pd.read_csv('assets/fn_kagg_train.csv') #assets/fn_kagg_train.csv\n",
    "data_news = data_news.dropna().reset_index(drop=True)\n",
    "data_news_cleanned = clean_up(data_news['text'])\n",
    "data_news['text_tokens'] = data_news_cleanned['text']\n",
    "data_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1b: Applying the Model\n",
    "\n",
    "Now, we want to predict the political bias of the target fake news dataset. We'll save these predictions as probabilities, which we'll use as additional features for clustering and trustworthiness prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                    author                      published  \\\n0        Barracuda Brigade  2016-10-26T21:41:00.000+03:00   \n1     reasoning with facts  2016-10-29T08:47:11.259+03:00   \n2        Barracuda Brigade  2016-10-31T01:41:49.479+02:00   \n4                   Fed Up  2016-11-01T21:56:00.000+02:00   \n5        Barracuda Brigade  2016-11-02T16:31:28.550+02:00   \n...                    ...                            ...   \n2040           Matt Barber  2016-10-27T03:04:50.327+03:00   \n2041         Jane Chastain  2016-10-27T03:04:50.704+03:00   \n2042         Michael Brown  2016-10-27T03:04:54.788+03:00   \n2043           Ann Coulter  2016-10-27T03:05:01.989+03:00   \n2044           Larry Elder  2016-10-27T03:05:05.815+03:00   \n\n                                                  title  \\\n0     muslims busted they stole millions in govt ben...   \n1     re why did attorney general loretta lynch plea...   \n2     breaking weiner cooperating with fbi on hillar...   \n4     fantastic trumps  point plan to reform healthc...   \n5     hillary goes absolutely berserk on protester a...   \n...                                                 ...   \n2040                 why never trumpers must reconsider   \n2041        election crossroads socialism or capitalism   \n2042                         reasons ill vote for trump   \n2043   our new country women and minorities hit hardest   \n2044              trump vs clinton a risk vs a disaster   \n\n                                                   text language  \\\n0     print they should pay all the back all the mon...  english   \n1     why did attorney general loretta lynch plead t...  english   \n2     red state  \\nfox news sunday reported this mor...  english   \n4     email healthcare reform to make america great ...  english   \n5     print hillary goes absolutely berserk she expl...  english   \n...                                                 ...      ...   \n2040  prof canoes reek of genocide white privilege c...  english   \n2041  teens walk free after gangrape conviction judg...  english   \n2042  school named for munichmassacre mastermind ter...  english   \n2043  wars and rumors of wars russia unveils satan  ...  english   \n2044  check out hillarythemed haunted house anticlin...  english   \n\n                 site_url                                       main_img_url  \\\n0     100percentfedup.com  http://bb4sp.com/wp-content/uploads/2016/10/Fu...   \n1     100percentfedup.com  http://bb4sp.com/wp-content/uploads/2016/10/Fu...   \n2     100percentfedup.com  http://bb4sp.com/wp-content/uploads/2016/10/Fu...   \n4     100percentfedup.com  http://100percentfedup.com/wp-content/uploads/...   \n5     100percentfedup.com  http://bb4sp.com/wp-content/uploads/2016/11/Fu...   \n...                   ...                                                ...   \n2040              wnd.com                                       No Image URL   \n2041              wnd.com                                       No Image URL   \n2042              wnd.com  http://mobile.wnd.com/files/2011/12/leftfield3...   \n2043              wnd.com  http://www.wnd.com/files/2016/10/danney-willll...   \n2044              wnd.com  http://www.wnd.com/files/2015/10/Hillary-Clint...   \n\n      type label                            title_without_stopwords  \\\n0     bias  Real        muslims busted stole millions govt benefits   \n1     bias  Real         attorney general loretta lynch plead fifth   \n2     bias  Real  breaking weiner cooperating fbi hillary email ...   \n4     bias  Real  fantastic trumps point plan reform healthcare ...   \n5     bias  Real  hillary goes absolutely berserk protester rall...   \n...    ...   ...                                                ...   \n2040  bias  Real                  trump vs clinton risk vs disaster   \n2041  bias  Real                    gingrich slutshames megyn kelly   \n2042  bias  Real                    youtube bans clintons black son   \n2043  bias  Real             wikileaks bombshells hillary need know   \n2044  bias  Real                                     fascinated sex   \n\n                                 text_without_stopwords  hasImage  \\\n0     print pay back money plus interest entire fami...       1.0   \n1     attorney general loretta lynch plead fifth bar...       1.0   \n2     red state fox news sunday reported morning ant...       1.0   \n4     email healthcare reform make america great sin...       1.0   \n5     print hillary goes absolutely berserk explodes...       1.0   \n...                                                 ...       ...   \n2040  check hillarythemed haunted house anticlinton ...       0.0   \n2041  good samaritan wearing indian headdress disarm...       1.0   \n2042  skype sex scam fortune built shame moroccan bo...       1.0   \n2043  posted eddie skyhigh potency may scare away cr...       1.0   \n2044  billion even known keeping supposedly deleted ...       0.0   \n\n                                            text_tokens  \n0     [print, pay, back, money, plus, interest, enti...  \n1     [attorney, general, loretta, lynch, plead, fif...  \n2     [red, state, fox, news, sunday, reported, morn...  \n4     [email, healthcare, reform, make, america, gre...  \n5     [print, hillary, go, absolutely, berserk, expl...  \n...                                                 ...  \n2040  [prof, canoe, reek, genocide, white, privilege...  \n2041  [teen, walk, free, gangrape, conviction, judge...  \n2042  [school, named, munichmassacre, mastermind, te...  \n2043  [war, rumor, war, russia, unveils, satan, miss...  \n2044  [check, hillarythemed, haunted, house, anticli...  \n\n[1732 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author</th>\n      <th>published</th>\n      <th>title</th>\n      <th>text</th>\n      <th>language</th>\n      <th>site_url</th>\n      <th>main_img_url</th>\n      <th>type</th>\n      <th>label</th>\n      <th>title_without_stopwords</th>\n      <th>text_without_stopwords</th>\n      <th>hasImage</th>\n      <th>text_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Barracuda Brigade</td>\n      <td>2016-10-26T21:41:00.000+03:00</td>\n      <td>muslims busted they stole millions in govt ben...</td>\n      <td>print they should pay all the back all the mon...</td>\n      <td>english</td>\n      <td>100percentfedup.com</td>\n      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>muslims busted stole millions govt benefits</td>\n      <td>print pay back money plus interest entire fami...</td>\n      <td>1.0</td>\n      <td>[print, pay, back, money, plus, interest, enti...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>reasoning with facts</td>\n      <td>2016-10-29T08:47:11.259+03:00</td>\n      <td>re why did attorney general loretta lynch plea...</td>\n      <td>why did attorney general loretta lynch plead t...</td>\n      <td>english</td>\n      <td>100percentfedup.com</td>\n      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>attorney general loretta lynch plead fifth</td>\n      <td>attorney general loretta lynch plead fifth bar...</td>\n      <td>1.0</td>\n      <td>[attorney, general, loretta, lynch, plead, fif...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Barracuda Brigade</td>\n      <td>2016-10-31T01:41:49.479+02:00</td>\n      <td>breaking weiner cooperating with fbi on hillar...</td>\n      <td>red state  \\nfox news sunday reported this mor...</td>\n      <td>english</td>\n      <td>100percentfedup.com</td>\n      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>breaking weiner cooperating fbi hillary email ...</td>\n      <td>red state fox news sunday reported morning ant...</td>\n      <td>1.0</td>\n      <td>[red, state, fox, news, sunday, reported, morn...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Fed Up</td>\n      <td>2016-11-01T21:56:00.000+02:00</td>\n      <td>fantastic trumps  point plan to reform healthc...</td>\n      <td>email healthcare reform to make america great ...</td>\n      <td>english</td>\n      <td>100percentfedup.com</td>\n      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>fantastic trumps point plan reform healthcare ...</td>\n      <td>email healthcare reform make america great sin...</td>\n      <td>1.0</td>\n      <td>[email, healthcare, reform, make, america, gre...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Barracuda Brigade</td>\n      <td>2016-11-02T16:31:28.550+02:00</td>\n      <td>hillary goes absolutely berserk on protester a...</td>\n      <td>print hillary goes absolutely berserk she expl...</td>\n      <td>english</td>\n      <td>100percentfedup.com</td>\n      <td>http://bb4sp.com/wp-content/uploads/2016/11/Fu...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>hillary goes absolutely berserk protester rall...</td>\n      <td>print hillary goes absolutely berserk explodes...</td>\n      <td>1.0</td>\n      <td>[print, hillary, go, absolutely, berserk, expl...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2040</th>\n      <td>Matt Barber</td>\n      <td>2016-10-27T03:04:50.327+03:00</td>\n      <td>why never trumpers must reconsider</td>\n      <td>prof canoes reek of genocide white privilege c...</td>\n      <td>english</td>\n      <td>wnd.com</td>\n      <td>No Image URL</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>trump vs clinton risk vs disaster</td>\n      <td>check hillarythemed haunted house anticlinton ...</td>\n      <td>0.0</td>\n      <td>[prof, canoe, reek, genocide, white, privilege...</td>\n    </tr>\n    <tr>\n      <th>2041</th>\n      <td>Jane Chastain</td>\n      <td>2016-10-27T03:04:50.704+03:00</td>\n      <td>election crossroads socialism or capitalism</td>\n      <td>teens walk free after gangrape conviction judg...</td>\n      <td>english</td>\n      <td>wnd.com</td>\n      <td>No Image URL</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>gingrich slutshames megyn kelly</td>\n      <td>good samaritan wearing indian headdress disarm...</td>\n      <td>1.0</td>\n      <td>[teen, walk, free, gangrape, conviction, judge...</td>\n    </tr>\n    <tr>\n      <th>2042</th>\n      <td>Michael Brown</td>\n      <td>2016-10-27T03:04:54.788+03:00</td>\n      <td>reasons ill vote for trump</td>\n      <td>school named for munichmassacre mastermind ter...</td>\n      <td>english</td>\n      <td>wnd.com</td>\n      <td>http://mobile.wnd.com/files/2011/12/leftfield3...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>youtube bans clintons black son</td>\n      <td>skype sex scam fortune built shame moroccan bo...</td>\n      <td>1.0</td>\n      <td>[school, named, munichmassacre, mastermind, te...</td>\n    </tr>\n    <tr>\n      <th>2043</th>\n      <td>Ann Coulter</td>\n      <td>2016-10-27T03:05:01.989+03:00</td>\n      <td>our new country women and minorities hit hardest</td>\n      <td>wars and rumors of wars russia unveils satan  ...</td>\n      <td>english</td>\n      <td>wnd.com</td>\n      <td>http://www.wnd.com/files/2016/10/danney-willll...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>wikileaks bombshells hillary need know</td>\n      <td>posted eddie skyhigh potency may scare away cr...</td>\n      <td>1.0</td>\n      <td>[war, rumor, war, russia, unveils, satan, miss...</td>\n    </tr>\n    <tr>\n      <th>2044</th>\n      <td>Larry Elder</td>\n      <td>2016-10-27T03:05:05.815+03:00</td>\n      <td>trump vs clinton a risk vs a disaster</td>\n      <td>check out hillarythemed haunted house anticlin...</td>\n      <td>english</td>\n      <td>wnd.com</td>\n      <td>http://www.wnd.com/files/2015/10/Hillary-Clint...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>fascinated sex</td>\n      <td>billion even known keeping supposedly deleted ...</td>\n      <td>0.0</td>\n      <td>[check, hillarythemed, haunted, house, anticli...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1732 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_kag_tok = data_news.copy()\n",
    "fn_kag_tok['tmp'] = fn_kag_tok['text_tokens'].apply(lambda x: len(x))\n",
    "fn_kag_tok = fn_kag_tok[fn_kag_tok['tmp']>30]\n",
    "fn_kag_tok = fn_kag_tok.drop(columns='tmp')\n",
    "fn_kag_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Now we'll apply the Word2Vec model we generated above to our tokens to vectorize the text.\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in fn_kag_tok.text_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Now we apply the Random Forest classifier to our vectorized text and save out the predicted probabilities.\n",
    "preds = pd.DataFrame(clf.predict_proba(vec_frame), columns=['dem_bias','neutral','rep_bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                    author                      published  \\\n0        Barracuda Brigade  2016-10-26T21:41:00.000+03:00   \n1     reasoning with facts  2016-10-29T08:47:11.259+03:00   \n2        Barracuda Brigade  2016-10-31T01:41:49.479+02:00   \n3                   Fed Up  2016-11-01T21:56:00.000+02:00   \n4        Barracuda Brigade  2016-11-02T16:31:28.550+02:00   \n...                    ...                            ...   \n1727           Matt Barber  2016-10-27T03:04:50.327+03:00   \n1728         Jane Chastain  2016-10-27T03:04:50.704+03:00   \n1729         Michael Brown  2016-10-27T03:04:54.788+03:00   \n1730           Ann Coulter  2016-10-27T03:05:01.989+03:00   \n1731           Larry Elder  2016-10-27T03:05:05.815+03:00   \n\n                                                  title  \\\n0     muslims busted they stole millions in govt ben...   \n1     re why did attorney general loretta lynch plea...   \n2     breaking weiner cooperating with fbi on hillar...   \n3     fantastic trumps  point plan to reform healthc...   \n4     hillary goes absolutely berserk on protester a...   \n...                                                 ...   \n1727                 why never trumpers must reconsider   \n1728        election crossroads socialism or capitalism   \n1729                         reasons ill vote for trump   \n1730   our new country women and minorities hit hardest   \n1731              trump vs clinton a risk vs a disaster   \n\n                                                   text language  \\\n0     print they should pay all the back all the mon...  english   \n1     why did attorney general loretta lynch plead t...  english   \n2     red state  \\nfox news sunday reported this mor...  english   \n3     email healthcare reform to make america great ...  english   \n4     print hillary goes absolutely berserk she expl...  english   \n...                                                 ...      ...   \n1727  prof canoes reek of genocide white privilege c...  english   \n1728  teens walk free after gangrape conviction judg...  english   \n1729  school named for munichmassacre mastermind ter...  english   \n1730  wars and rumors of wars russia unveils satan  ...  english   \n1731  check out hillarythemed haunted house anticlin...  english   \n\n                 site_url                                       main_img_url  \\\n0     100percentfedup.com  http://bb4sp.com/wp-content/uploads/2016/10/Fu...   \n1     100percentfedup.com  http://bb4sp.com/wp-content/uploads/2016/10/Fu...   \n2     100percentfedup.com  http://bb4sp.com/wp-content/uploads/2016/10/Fu...   \n3     100percentfedup.com  http://100percentfedup.com/wp-content/uploads/...   \n4     100percentfedup.com  http://bb4sp.com/wp-content/uploads/2016/11/Fu...   \n...                   ...                                                ...   \n1727              wnd.com                                       No Image URL   \n1728              wnd.com                                       No Image URL   \n1729              wnd.com  http://mobile.wnd.com/files/2011/12/leftfield3...   \n1730              wnd.com  http://www.wnd.com/files/2016/10/danney-willll...   \n1731              wnd.com  http://www.wnd.com/files/2015/10/Hillary-Clint...   \n\n      type label                            title_without_stopwords  \\\n0     bias  Real        muslims busted stole millions govt benefits   \n1     bias  Real         attorney general loretta lynch plead fifth   \n2     bias  Real  breaking weiner cooperating fbi hillary email ...   \n3     bias  Real  fantastic trumps point plan reform healthcare ...   \n4     bias  Real  hillary goes absolutely berserk protester rall...   \n...    ...   ...                                                ...   \n1727  bias  Real                  trump vs clinton risk vs disaster   \n1728  bias  Real                    gingrich slutshames megyn kelly   \n1729  bias  Real                    youtube bans clintons black son   \n1730  bias  Real             wikileaks bombshells hillary need know   \n1731  bias  Real                                     fascinated sex   \n\n                                 text_without_stopwords  hasImage  \\\n0     print pay back money plus interest entire fami...       1.0   \n1     attorney general loretta lynch plead fifth bar...       1.0   \n2     red state fox news sunday reported morning ant...       1.0   \n3     email healthcare reform make america great sin...       1.0   \n4     print hillary goes absolutely berserk explodes...       1.0   \n...                                                 ...       ...   \n1727  check hillarythemed haunted house anticlinton ...       0.0   \n1728  good samaritan wearing indian headdress disarm...       1.0   \n1729  skype sex scam fortune built shame moroccan bo...       1.0   \n1730  posted eddie skyhigh potency may scare away cr...       1.0   \n1731  billion even known keeping supposedly deleted ...       0.0   \n\n                                            text_tokens  dem_bias  neutral  \\\n0     [print, pay, back, money, plus, interest, enti...  0.370000     0.14   \n1     [attorney, general, loretta, lynch, plead, fif...  0.200000     0.19   \n2     [red, state, fox, news, sunday, reported, morn...  0.497333     0.06   \n3     [email, healthcare, reform, make, america, gre...  0.400000     0.10   \n4     [print, hillary, go, absolutely, berserk, expl...  0.530000     0.07   \n...                                                 ...       ...      ...   \n1727  [prof, canoe, reek, genocide, white, privilege...  0.570000     0.12   \n1728  [teen, walk, free, gangrape, conviction, judge...  0.350000     0.25   \n1729  [school, named, munichmassacre, mastermind, te...  0.260000     0.19   \n1730  [war, rumor, war, russia, unveils, satan, miss...  0.270000     0.45   \n1731  [check, hillarythemed, haunted, house, anticli...  0.230000     0.18   \n\n      rep_bias  \n0     0.490000  \n1     0.610000  \n2     0.442667  \n3     0.500000  \n4     0.400000  \n...        ...  \n1727  0.310000  \n1728  0.400000  \n1729  0.550000  \n1730  0.280000  \n1731  0.590000  \n\n[1732 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author</th>\n      <th>published</th>\n      <th>title</th>\n      <th>text</th>\n      <th>language</th>\n      <th>site_url</th>\n      <th>main_img_url</th>\n      <th>type</th>\n      <th>label</th>\n      <th>title_without_stopwords</th>\n      <th>text_without_stopwords</th>\n      <th>hasImage</th>\n      <th>text_tokens</th>\n      <th>dem_bias</th>\n      <th>neutral</th>\n      <th>rep_bias</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Barracuda Brigade</td>\n      <td>2016-10-26T21:41:00.000+03:00</td>\n      <td>muslims busted they stole millions in govt ben...</td>\n      <td>print they should pay all the back all the mon...</td>\n      <td>english</td>\n      <td>100percentfedup.com</td>\n      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>muslims busted stole millions govt benefits</td>\n      <td>print pay back money plus interest entire fami...</td>\n      <td>1.0</td>\n      <td>[print, pay, back, money, plus, interest, enti...</td>\n      <td>0.370000</td>\n      <td>0.14</td>\n      <td>0.490000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>reasoning with facts</td>\n      <td>2016-10-29T08:47:11.259+03:00</td>\n      <td>re why did attorney general loretta lynch plea...</td>\n      <td>why did attorney general loretta lynch plead t...</td>\n      <td>english</td>\n      <td>100percentfedup.com</td>\n      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>attorney general loretta lynch plead fifth</td>\n      <td>attorney general loretta lynch plead fifth bar...</td>\n      <td>1.0</td>\n      <td>[attorney, general, loretta, lynch, plead, fif...</td>\n      <td>0.200000</td>\n      <td>0.19</td>\n      <td>0.610000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Barracuda Brigade</td>\n      <td>2016-10-31T01:41:49.479+02:00</td>\n      <td>breaking weiner cooperating with fbi on hillar...</td>\n      <td>red state  \\nfox news sunday reported this mor...</td>\n      <td>english</td>\n      <td>100percentfedup.com</td>\n      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>breaking weiner cooperating fbi hillary email ...</td>\n      <td>red state fox news sunday reported morning ant...</td>\n      <td>1.0</td>\n      <td>[red, state, fox, news, sunday, reported, morn...</td>\n      <td>0.497333</td>\n      <td>0.06</td>\n      <td>0.442667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Fed Up</td>\n      <td>2016-11-01T21:56:00.000+02:00</td>\n      <td>fantastic trumps  point plan to reform healthc...</td>\n      <td>email healthcare reform to make america great ...</td>\n      <td>english</td>\n      <td>100percentfedup.com</td>\n      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>fantastic trumps point plan reform healthcare ...</td>\n      <td>email healthcare reform make america great sin...</td>\n      <td>1.0</td>\n      <td>[email, healthcare, reform, make, america, gre...</td>\n      <td>0.400000</td>\n      <td>0.10</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Barracuda Brigade</td>\n      <td>2016-11-02T16:31:28.550+02:00</td>\n      <td>hillary goes absolutely berserk on protester a...</td>\n      <td>print hillary goes absolutely berserk she expl...</td>\n      <td>english</td>\n      <td>100percentfedup.com</td>\n      <td>http://bb4sp.com/wp-content/uploads/2016/11/Fu...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>hillary goes absolutely berserk protester rall...</td>\n      <td>print hillary goes absolutely berserk explodes...</td>\n      <td>1.0</td>\n      <td>[print, hillary, go, absolutely, berserk, expl...</td>\n      <td>0.530000</td>\n      <td>0.07</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1727</th>\n      <td>Matt Barber</td>\n      <td>2016-10-27T03:04:50.327+03:00</td>\n      <td>why never trumpers must reconsider</td>\n      <td>prof canoes reek of genocide white privilege c...</td>\n      <td>english</td>\n      <td>wnd.com</td>\n      <td>No Image URL</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>trump vs clinton risk vs disaster</td>\n      <td>check hillarythemed haunted house anticlinton ...</td>\n      <td>0.0</td>\n      <td>[prof, canoe, reek, genocide, white, privilege...</td>\n      <td>0.570000</td>\n      <td>0.12</td>\n      <td>0.310000</td>\n    </tr>\n    <tr>\n      <th>1728</th>\n      <td>Jane Chastain</td>\n      <td>2016-10-27T03:04:50.704+03:00</td>\n      <td>election crossroads socialism or capitalism</td>\n      <td>teens walk free after gangrape conviction judg...</td>\n      <td>english</td>\n      <td>wnd.com</td>\n      <td>No Image URL</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>gingrich slutshames megyn kelly</td>\n      <td>good samaritan wearing indian headdress disarm...</td>\n      <td>1.0</td>\n      <td>[teen, walk, free, gangrape, conviction, judge...</td>\n      <td>0.350000</td>\n      <td>0.25</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <th>1729</th>\n      <td>Michael Brown</td>\n      <td>2016-10-27T03:04:54.788+03:00</td>\n      <td>reasons ill vote for trump</td>\n      <td>school named for munichmassacre mastermind ter...</td>\n      <td>english</td>\n      <td>wnd.com</td>\n      <td>http://mobile.wnd.com/files/2011/12/leftfield3...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>youtube bans clintons black son</td>\n      <td>skype sex scam fortune built shame moroccan bo...</td>\n      <td>1.0</td>\n      <td>[school, named, munichmassacre, mastermind, te...</td>\n      <td>0.260000</td>\n      <td>0.19</td>\n      <td>0.550000</td>\n    </tr>\n    <tr>\n      <th>1730</th>\n      <td>Ann Coulter</td>\n      <td>2016-10-27T03:05:01.989+03:00</td>\n      <td>our new country women and minorities hit hardest</td>\n      <td>wars and rumors of wars russia unveils satan  ...</td>\n      <td>english</td>\n      <td>wnd.com</td>\n      <td>http://www.wnd.com/files/2016/10/danney-willll...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>wikileaks bombshells hillary need know</td>\n      <td>posted eddie skyhigh potency may scare away cr...</td>\n      <td>1.0</td>\n      <td>[war, rumor, war, russia, unveils, satan, miss...</td>\n      <td>0.270000</td>\n      <td>0.45</td>\n      <td>0.280000</td>\n    </tr>\n    <tr>\n      <th>1731</th>\n      <td>Larry Elder</td>\n      <td>2016-10-27T03:05:05.815+03:00</td>\n      <td>trump vs clinton a risk vs a disaster</td>\n      <td>check out hillarythemed haunted house anticlin...</td>\n      <td>english</td>\n      <td>wnd.com</td>\n      <td>http://www.wnd.com/files/2015/10/Hillary-Clint...</td>\n      <td>bias</td>\n      <td>Real</td>\n      <td>fascinated sex</td>\n      <td>billion even known keeping supposedly deleted ...</td>\n      <td>0.0</td>\n      <td>[check, hillarythemed, haunted, house, anticli...</td>\n      <td>0.230000</td>\n      <td>0.18</td>\n      <td>0.590000</td>\n    </tr>\n  </tbody>\n</table>\n<p>1732 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally, we'll rejoin the predictions to the original dataset.\n",
    "fn_kag_reduced = fn_kag_tok.copy().reset_index(drop=True)\n",
    "fn_kag_reduced = fn_kag_reduced.join(preds)\n",
    "fn_kag_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2: Clustering\n",
    "Once we have all the features we want, we'll do unsupervised clustering. Ideally we'd want to do some evaluations to find an ideal number of clusters, but for now we'll just go with 4.\n",
    "\n",
    "We'll need to re-vectorize the text, as the political bias vectors won't work here. Also, we'd probably want to vectorize both headline and article body, but for now I'll just vectorize the article body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#From this point on, we should be concerned with data leakage. Everything prior to now could in theory be applied to live data. We'll go ahead and split the data out into train and test sets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(fn_kag_reduced.drop(columns=['label']), fn_kag_reduced.label, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Since we already have the tokenized text from above, we can just go ahead and train the new Word2Vec model on those tokens.\n",
    "wv_mod = Word2Vec(X_train['text_tokens'], seed = RANDOM_SEED)\n",
    "wv_mod.save(\"models/fn_w2v_model.pkl\") #models/fn_w2v_model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Again we'll extract and average the word vectors.\n",
    "vectors = wv_mod.wv\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in X_train.text_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#We'll join the new word vectors with the bias estimates we generate above.\n",
    "X_train_all = vec_frame.join(X_train).drop(columns=['id','title','author','text','text_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Finally we'll build our clustering model...\n",
    "cls = KMeans(4, random_state=RANDOM_SEED).fit(X_train_all)\n",
    "filename = \"models/cluster_mod.pkl\"\n",
    "pickle.dump(cls, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#...and add the predicted clusters back into the vector dataframe.\n",
    "X_train_all['cluster'] = cls.predict(X_train_all)\n",
    "X_train_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 3: Supervised Learning\n",
    "\n",
    "Now that we have all of our features and clusters, and article body text is already vectorized, we can train a classifier to predict whether a given article is misinformation or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>dem_bias</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rep_bias</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.004069</td>\n",
       "      <td>0.006721</td>\n",
       "      <td>0.025754</td>\n",
       "      <td>0.016396</td>\n",
       "      <td>-0.024229</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>-0.005376</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>-0.014789</td>\n",
       "      <td>-0.061167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.015933</td>\n",
       "      <td>0.034293</td>\n",
       "      <td>0.010748</td>\n",
       "      <td>-0.005926</td>\n",
       "      <td>0.024244</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007615</td>\n",
       "      <td>0.021091</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>0.035196</td>\n",
       "      <td>-0.026442</td>\n",
       "      <td>0.019393</td>\n",
       "      <td>0.025404</td>\n",
       "      <td>0.076108</td>\n",
       "      <td>-0.016611</td>\n",
       "      <td>-0.025038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039519</td>\n",
       "      <td>0.040006</td>\n",
       "      <td>0.030866</td>\n",
       "      <td>0.032353</td>\n",
       "      <td>-0.008771</td>\n",
       "      <td>-0.000636</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.015921</td>\n",
       "      <td>-0.017781</td>\n",
       "      <td>0.027126</td>\n",
       "      <td>0.039706</td>\n",
       "      <td>-0.072932</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>0.049133</td>\n",
       "      <td>-0.028249</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>-0.090377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.026980</td>\n",
       "      <td>-0.000872</td>\n",
       "      <td>0.036489</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>-0.001185</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026510</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>-0.015315</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>-0.045030</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>-0.010983</td>\n",
       "      <td>-0.036082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050313</td>\n",
       "      <td>0.056651</td>\n",
       "      <td>-0.007929</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>-0.000486</td>\n",
       "      <td>0.025396</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.030441</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>0.020871</td>\n",
       "      <td>0.011121</td>\n",
       "      <td>-0.068780</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>0.026041</td>\n",
       "      <td>0.025270</td>\n",
       "      <td>0.028207</td>\n",
       "      <td>-0.053540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010415</td>\n",
       "      <td>0.031614</td>\n",
       "      <td>0.007730</td>\n",
       "      <td>0.036058</td>\n",
       "      <td>0.023472</td>\n",
       "      <td>0.025822</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3494</th>\n",
       "      <td>0.000622</td>\n",
       "      <td>-0.015777</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>0.022281</td>\n",
       "      <td>-0.019934</td>\n",
       "      <td>0.022051</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.027889</td>\n",
       "      <td>-0.018690</td>\n",
       "      <td>-0.037581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043006</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>0.006654</td>\n",
       "      <td>0.009844</td>\n",
       "      <td>-0.018182</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>-0.003556</td>\n",
       "      <td>-0.001852</td>\n",
       "      <td>0.027409</td>\n",
       "      <td>0.018975</td>\n",
       "      <td>-0.044493</td>\n",
       "      <td>-0.012646</td>\n",
       "      <td>-0.015225</td>\n",
       "      <td>-0.051780</td>\n",
       "      <td>-0.019676</td>\n",
       "      <td>-0.039484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017797</td>\n",
       "      <td>0.033542</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>-0.028908</td>\n",
       "      <td>-0.022689</td>\n",
       "      <td>-0.023097</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>-0.005016</td>\n",
       "      <td>-0.026904</td>\n",
       "      <td>-0.028986</td>\n",
       "      <td>-0.009276</td>\n",
       "      <td>-0.035803</td>\n",
       "      <td>-0.053517</td>\n",
       "      <td>0.012711</td>\n",
       "      <td>-0.000715</td>\n",
       "      <td>-0.050154</td>\n",
       "      <td>-0.062212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025777</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>-0.005290</td>\n",
       "      <td>0.012638</td>\n",
       "      <td>-0.013910</td>\n",
       "      <td>-0.047990</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>-0.003582</td>\n",
       "      <td>-0.017635</td>\n",
       "      <td>0.022182</td>\n",
       "      <td>0.072596</td>\n",
       "      <td>-0.027446</td>\n",
       "      <td>0.024533</td>\n",
       "      <td>-0.008718</td>\n",
       "      <td>0.059137</td>\n",
       "      <td>-0.027213</td>\n",
       "      <td>-0.041577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051082</td>\n",
       "      <td>0.046678</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.037541</td>\n",
       "      <td>-0.034289</td>\n",
       "      <td>-0.009565</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>-0.061865</td>\n",
       "      <td>-0.025476</td>\n",
       "      <td>0.023372</td>\n",
       "      <td>0.063323</td>\n",
       "      <td>-0.034211</td>\n",
       "      <td>0.012190</td>\n",
       "      <td>-0.025201</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.006571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031042</td>\n",
       "      <td>0.040054</td>\n",
       "      <td>-0.017295</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>-0.016122</td>\n",
       "      <td>-0.032334</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3499 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.004069  0.006721  0.025754  0.016396 -0.024229  0.014822 -0.005376   \n",
       "1     0.007615  0.021091  0.009161  0.035196 -0.026442  0.019393  0.025404   \n",
       "2    -0.015921 -0.017781  0.027126  0.039706 -0.072932 -0.000181  0.049133   \n",
       "3     0.026510  0.011171 -0.015315  0.002247  0.002525  0.007189 -0.045030   \n",
       "4     0.030441  0.015128  0.020871  0.011121 -0.068780  0.009686  0.026041   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3494  0.000622 -0.015777  0.001780  0.022281 -0.019934  0.022051  0.005040   \n",
       "3495 -0.003556 -0.001852  0.027409  0.018975 -0.044493 -0.012646 -0.015225   \n",
       "3496 -0.005016 -0.026904 -0.028986 -0.009276 -0.035803 -0.053517  0.012711   \n",
       "3497 -0.003582 -0.017635  0.022182  0.072596 -0.027446  0.024533 -0.008718   \n",
       "3498 -0.061865 -0.025476  0.023372  0.063323 -0.034211  0.012190 -0.025201   \n",
       "\n",
       "             7         8         9  ...        94        95        96  \\\n",
       "0     0.004720 -0.014789 -0.061167  ...  0.000795  0.015933  0.034293   \n",
       "1     0.076108 -0.016611 -0.025038  ...  0.039519  0.040006  0.030866   \n",
       "2    -0.028249  0.002583 -0.090377  ...  0.006657  0.026980 -0.000872   \n",
       "3     0.004801 -0.010983 -0.036082  ...  0.050313  0.056651 -0.007929   \n",
       "4     0.025270  0.028207 -0.053540  ...  0.010415  0.031614  0.007730   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3494  0.027889 -0.018690 -0.037581  ...  0.043006  0.008060  0.006654   \n",
       "3495 -0.051780 -0.019676 -0.039484  ...  0.017797  0.033542  0.000850   \n",
       "3496 -0.000715 -0.050154 -0.062212  ...  0.025777 -0.008230 -0.005290   \n",
       "3497  0.059137 -0.027213 -0.041577  ...  0.051082  0.046678  0.002067   \n",
       "3498  0.016879  0.003855  0.006571  ...  0.031042  0.040054 -0.017295   \n",
       "\n",
       "            97        98        99  dem_bias  neutral  rep_bias  cluster  \n",
       "0     0.010748 -0.005926  0.024244      0.54     0.08      0.38        2  \n",
       "1     0.032353 -0.008771 -0.000636      0.59     0.02      0.39        2  \n",
       "2     0.036489  0.011354 -0.001185      0.29     0.30      0.41        1  \n",
       "3     0.030120 -0.000486  0.025396      0.32     0.18      0.50        1  \n",
       "4     0.036058  0.023472  0.025822      0.48     0.07      0.45        2  \n",
       "...        ...       ...       ...       ...      ...       ...      ...  \n",
       "3494  0.009844 -0.018182  0.003670      0.54     0.06      0.40        2  \n",
       "3495 -0.028908 -0.022689 -0.023097      0.12     0.28      0.60        1  \n",
       "3496  0.012638 -0.013910 -0.047990      0.13     0.24      0.63        1  \n",
       "3497  0.037541 -0.034289 -0.009565      0.56     0.06      0.38        2  \n",
       "3498  0.000189 -0.016122 -0.032334      0.12     0.40      0.48        1  \n",
       "\n",
       "[3499 rows x 104 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to apply the vectorization and clustering from above to the test data.\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in X_test.text_tokens])\n",
    "X_test_all = vec_frame.join(X_test).drop(columns=['id','title','author','text','text_tokens'])\n",
    "X_test_all['cluster'] = cls.predict(X_test_all)\n",
    "X_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8939697056301801"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "clf.fit(X_train_all, y_train)\n",
    "filename = \"models/fn_classifier_model.pkl\"\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "clf.score(X_test_all, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 4: Export CSV\n",
    "\n",
    "So now we have 2 datasets (3 if you count the unused 'test' set from the Kaggle fake news data). We want to have 1 large dataset to power the dashboard. Let's load each dataset into a dataframe, vectorize the text, predict a cluster, and predict whether it's misinformation or not. We'll start fresh to keep things simple, and we'll just use article body text.\n",
    "\n",
    "This part can really stand on its own... should probably pull it out into a separate notebook/script? Only issue is we'd have to repeat the code for the NLP steps, unless we save them into their own script as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Load each datafile we want to process\n",
    "data_bias=pd.read_excel('assets/pb_spinde.xlsx')\n",
    "data_news_1=pd.read_csv('assets/fn_kagg_train.csv')\n",
    "data_news_2=pd.read_csv('assets/fn_kagg_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Load all of our models\n",
    "pb_vec_mod = Word2Vec.load('models/pb_w2v_model.pkl')\n",
    "fn_vec_mod = Word2Vec.load('models/fn_w2v_model.pkl')\n",
    "cls = pickle.load(open('models/cluster_mod.pkl', 'rb'))\n",
    "pb_clf = pickle.load(open('models/pb_classifier_model.pkl', 'rb'))\n",
    "fn_clf = pickle.load(open('models/fn_classifier_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27549</th>\n",
       "      <td>Of all the dysfunctions that plague the world’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27550</th>\n",
       "      <td>WASHINGTON  —   Gov. John Kasich of Ohio on Tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27551</th>\n",
       "      <td>Good morning. (Want to get California Today by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27552</th>\n",
       "      <td>« Previous - Next » 300 US Marines To Be Deplo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27553</th>\n",
       "      <td>Perhaps you’ve seen the new TV series whose pi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27554 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               full_text\n",
       "0      YouTube says no ‘deepfakes’ or ‘birther’ video...\n",
       "1      FRISCO, Texas — The increasingly bitter disput...\n",
       "2      Speaking to the country for the first time fro...\n",
       "3      A professor who teaches climate change classes...\n",
       "4      The left has a thing for taking babies hostage...\n",
       "...                                                  ...\n",
       "27549  Of all the dysfunctions that plague the world’...\n",
       "27550  WASHINGTON  —   Gov. John Kasich of Ohio on Tu...\n",
       "27551  Good morning. (Want to get California Today by...\n",
       "27552  « Previous - Next » 300 US Marines To Be Deplo...\n",
       "27553  Perhaps you’ve seen the new TV series whose pi...\n",
       "\n",
       "[27554 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grab just the article text, drop empty cells, and stack the dataframes.\n",
    "db_text = data_bias.article\n",
    "dn1_text = data_news_1.text\n",
    "dn2_text = data_news_2.text\n",
    "full_data = pd.DataFrame(pd.concat([db_text, dn1_text, dn2_text], axis = 0).dropna().reset_index(drop=True))\n",
    "full_data.columns = ['full_text']\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>[youtube, says, deepfakes, birther, videos, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "      <td>[frisco, texas, increasingly, bitter, dispute,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>[speaking, country, first, time, oval, office,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>[professor, teaches, climate, change, classes,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>[left, thing, taking, babies, hostage, perfect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25834</th>\n",
       "      <td>Of all the dysfunctions that plague the world’...</td>\n",
       "      <td>[dysfunctions, plague, worlds, megacities, non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25835</th>\n",
       "      <td>WASHINGTON  —   Gov. John Kasich of Ohio on Tu...</td>\n",
       "      <td>[washington, gov, john, kasich, ohio, tuesday,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25836</th>\n",
       "      <td>Good morning. (Want to get California Today by...</td>\n",
       "      <td>[good, morning, want, get, california, today, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25837</th>\n",
       "      <td>« Previous - Next » 300 US Marines To Be Deplo...</td>\n",
       "      <td>[previous, next, us, marines, deployed, russia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25838</th>\n",
       "      <td>Perhaps you’ve seen the new TV series whose pi...</td>\n",
       "      <td>[perhaps, youve, seen, new, tv, series, whose,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25839 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               full_text  \\\n",
       "0      YouTube says no ‘deepfakes’ or ‘birther’ video...   \n",
       "1      FRISCO, Texas — The increasingly bitter disput...   \n",
       "2      Speaking to the country for the first time fro...   \n",
       "3      A professor who teaches climate change classes...   \n",
       "4      The left has a thing for taking babies hostage...   \n",
       "...                                                  ...   \n",
       "25834  Of all the dysfunctions that plague the world’...   \n",
       "25835  WASHINGTON  —   Gov. John Kasich of Ohio on Tu...   \n",
       "25836  Good morning. (Want to get California Today by...   \n",
       "25837  « Previous - Next » 300 US Marines To Be Deplo...   \n",
       "25838  Perhaps you’ve seen the new TV series whose pi...   \n",
       "\n",
       "                                                  tokens  \n",
       "0      [youtube, says, deepfakes, birther, videos, to...  \n",
       "1      [frisco, texas, increasingly, bitter, dispute,...  \n",
       "2      [speaking, country, first, time, oval, office,...  \n",
       "3      [professor, teaches, climate, change, classes,...  \n",
       "4      [left, thing, taking, babies, hostage, perfect...  \n",
       "...                                                  ...  \n",
       "25834  [dysfunctions, plague, worlds, megacities, non...  \n",
       "25835  [washington, gov, john, kasich, ohio, tuesday,...  \n",
       "25836  [good, morning, want, get, california, today, ...  \n",
       "25837  [previous, next, us, marines, deployed, russia...  \n",
       "25838  [perhaps, youve, seen, new, tv, series, whose,...  \n",
       "\n",
       "[25839 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we'll apply all the NLP steps from above to the full series. This can take a while. I'm skipping the lemmatization and stemming for now...\n",
    "# full_data_clean = full_data.copy()\n",
    "# full_data_clean['processing'] = full_data_clean.full_text.apply(lambda x: x.encode('ascii', 'ignore').decode(\"ascii\",\"ignore\"))\n",
    "# full_data_clean['processing'] = full_data_clean.processing.apply(round1)\n",
    "# full_data_clean['processing'] = full_data_clean.processing.apply(round2)\n",
    "# full_data_clean['processing'] = full_data_clean.processing.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "# full_data_clean['tokens'] = full_data_clean.processing.apply(lambda y: [x for x in word_tokenize(y)])\n",
    "\n",
    "# full_data_clean['tmp'] = full_data_clean['tokens'].apply(lambda x: len(x))\n",
    "# full_data_clean = full_data_clean[full_data_clean['tmp']>30]\n",
    "# full_data_clean = full_data_clean.drop(columns=['tmp','processing'])\n",
    "\n",
    "# full_data_clean.to_pickle('clean_full_data.pkl')\n",
    "full_data_clean = pd.read_pickle('clean_full_data.pkl').reset_index(drop=True)\n",
    "full_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>dem_bias</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rep_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>[youtube, says, deepfakes, birther, videos, to...</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "      <td>[frisco, texas, increasingly, bitter, dispute,...</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>[speaking, country, first, time, oval, office,...</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>[professor, teaches, climate, change, classes,...</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>[left, thing, taking, babies, hostage, perfect...</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25834</th>\n",
       "      <td>Of all the dysfunctions that plague the world’...</td>\n",
       "      <td>[dysfunctions, plague, worlds, megacities, non...</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25835</th>\n",
       "      <td>WASHINGTON  —   Gov. John Kasich of Ohio on Tu...</td>\n",
       "      <td>[washington, gov, john, kasich, ohio, tuesday,...</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25836</th>\n",
       "      <td>Good morning. (Want to get California Today by...</td>\n",
       "      <td>[good, morning, want, get, california, today, ...</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25837</th>\n",
       "      <td>« Previous - Next » 300 US Marines To Be Deplo...</td>\n",
       "      <td>[previous, next, us, marines, deployed, russia...</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.623077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25838</th>\n",
       "      <td>Perhaps you’ve seen the new TV series whose pi...</td>\n",
       "      <td>[perhaps, youve, seen, new, tv, series, whose,...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25839 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               full_text  \\\n",
       "0      YouTube says no ‘deepfakes’ or ‘birther’ video...   \n",
       "1      FRISCO, Texas — The increasingly bitter disput...   \n",
       "2      Speaking to the country for the first time fro...   \n",
       "3      A professor who teaches climate change classes...   \n",
       "4      The left has a thing for taking babies hostage...   \n",
       "...                                                  ...   \n",
       "25834  Of all the dysfunctions that plague the world’...   \n",
       "25835  WASHINGTON  —   Gov. John Kasich of Ohio on Tu...   \n",
       "25836  Good morning. (Want to get California Today by...   \n",
       "25837  « Previous - Next » 300 US Marines To Be Deplo...   \n",
       "25838  Perhaps you’ve seen the new TV series whose pi...   \n",
       "\n",
       "                                                  tokens  dem_bias  neutral  \\\n",
       "0      [youtube, says, deepfakes, birther, videos, to...  0.350000     0.33   \n",
       "1      [frisco, texas, increasingly, bitter, dispute,...  0.490000     0.12   \n",
       "2      [speaking, country, first, time, oval, office,...  0.730000     0.08   \n",
       "3      [professor, teaches, climate, change, classes,...  0.520000     0.02   \n",
       "4      [left, thing, taking, babies, hostage, perfect...  0.140000     0.01   \n",
       "...                                                  ...       ...      ...   \n",
       "25834  [dysfunctions, plague, worlds, megacities, non...  0.310000     0.06   \n",
       "25835  [washington, gov, john, kasich, ohio, tuesday,...  0.320000     0.38   \n",
       "25836  [good, morning, want, get, california, today, ...  0.190000     0.18   \n",
       "25837  [previous, next, us, marines, deployed, russia...  0.326923     0.05   \n",
       "25838  [perhaps, youve, seen, new, tv, series, whose,...  0.160000     0.01   \n",
       "\n",
       "       rep_bias  \n",
       "0      0.320000  \n",
       "1      0.390000  \n",
       "2      0.190000  \n",
       "3      0.460000  \n",
       "4      0.850000  \n",
       "...         ...  \n",
       "25834  0.630000  \n",
       "25835  0.300000  \n",
       "25836  0.630000  \n",
       "25837  0.623077  \n",
       "25838  0.830000  \n",
       "\n",
       "[25839 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we'll use the pb vectorizer and generate political bias predictions for the whole dataset.\n",
    "vectors = pb_vec_mod.wv\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in full_data_clean.tokens])\n",
    "preds = pd.DataFrame(pb_clf.predict_proba(vec_frame), columns=['dem_bias','neutral','rep_bias'])\n",
    "full_data_clean = full_data_clean.join(preds)\n",
    "full_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>dem_bias</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rep_bias</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>[youtube, says, deepfakes, birther, videos, to...</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "      <td>[frisco, texas, increasingly, bitter, dispute,...</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>[speaking, country, first, time, oval, office,...</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>[professor, teaches, climate, change, classes,...</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>[left, thing, taking, babies, hostage, perfect...</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25834</th>\n",
       "      <td>Of all the dysfunctions that plague the world’...</td>\n",
       "      <td>[dysfunctions, plague, worlds, megacities, non...</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25835</th>\n",
       "      <td>WASHINGTON  —   Gov. John Kasich of Ohio on Tu...</td>\n",
       "      <td>[washington, gov, john, kasich, ohio, tuesday,...</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25836</th>\n",
       "      <td>Good morning. (Want to get California Today by...</td>\n",
       "      <td>[good, morning, want, get, california, today, ...</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25837</th>\n",
       "      <td>« Previous - Next » 300 US Marines To Be Deplo...</td>\n",
       "      <td>[previous, next, us, marines, deployed, russia...</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.623077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25838</th>\n",
       "      <td>Perhaps you’ve seen the new TV series whose pi...</td>\n",
       "      <td>[perhaps, youve, seen, new, tv, series, whose,...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25839 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               full_text  \\\n",
       "0      YouTube says no ‘deepfakes’ or ‘birther’ video...   \n",
       "1      FRISCO, Texas — The increasingly bitter disput...   \n",
       "2      Speaking to the country for the first time fro...   \n",
       "3      A professor who teaches climate change classes...   \n",
       "4      The left has a thing for taking babies hostage...   \n",
       "...                                                  ...   \n",
       "25834  Of all the dysfunctions that plague the world’...   \n",
       "25835  WASHINGTON  —   Gov. John Kasich of Ohio on Tu...   \n",
       "25836  Good morning. (Want to get California Today by...   \n",
       "25837  « Previous - Next » 300 US Marines To Be Deplo...   \n",
       "25838  Perhaps you’ve seen the new TV series whose pi...   \n",
       "\n",
       "                                                  tokens  dem_bias  neutral  \\\n",
       "0      [youtube, says, deepfakes, birther, videos, to...  0.350000     0.33   \n",
       "1      [frisco, texas, increasingly, bitter, dispute,...  0.490000     0.12   \n",
       "2      [speaking, country, first, time, oval, office,...  0.730000     0.08   \n",
       "3      [professor, teaches, climate, change, classes,...  0.520000     0.02   \n",
       "4      [left, thing, taking, babies, hostage, perfect...  0.140000     0.01   \n",
       "...                                                  ...       ...      ...   \n",
       "25834  [dysfunctions, plague, worlds, megacities, non...  0.310000     0.06   \n",
       "25835  [washington, gov, john, kasich, ohio, tuesday,...  0.320000     0.38   \n",
       "25836  [good, morning, want, get, california, today, ...  0.190000     0.18   \n",
       "25837  [previous, next, us, marines, deployed, russia...  0.326923     0.05   \n",
       "25838  [perhaps, youve, seen, new, tv, series, whose,...  0.160000     0.01   \n",
       "\n",
       "       rep_bias  cluster  \n",
       "0      0.320000        1  \n",
       "1      0.390000        2  \n",
       "2      0.190000        2  \n",
       "3      0.460000        2  \n",
       "4      0.850000        0  \n",
       "...         ...      ...  \n",
       "25834  0.630000        0  \n",
       "25835  0.300000        1  \n",
       "25836  0.630000        0  \n",
       "25837  0.623077        0  \n",
       "25838  0.830000        0  \n",
       "\n",
       "[25839 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we'll predict clusters using the fake news w2v model.\n",
    "vectors = fn_vec_mod.wv\n",
    "vec_frame = pd.DataFrame([vectors.get_mean_vector(x) for x in full_data_clean.tokens])\n",
    "cluster_frame = vec_frame.join(full_data_clean).drop(columns=['full_text','tokens'])\n",
    "full_data_clean['cluster'] = cls.predict(cluster_frame)\n",
    "full_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>dem_bias</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rep_bias</th>\n",
       "      <th>cluster</th>\n",
       "      <th>not_misinfo</th>\n",
       "      <th>misinfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>[youtube, says, deepfakes, birther, videos, to...</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "      <td>[frisco, texas, increasingly, bitter, dispute,...</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>[speaking, country, first, time, oval, office,...</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>[professor, teaches, climate, change, classes,...</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>[left, thing, taking, babies, hostage, perfect...</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25834</th>\n",
       "      <td>Of all the dysfunctions that plague the world’...</td>\n",
       "      <td>[dysfunctions, plague, worlds, megacities, non...</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25835</th>\n",
       "      <td>WASHINGTON  —   Gov. John Kasich of Ohio on Tu...</td>\n",
       "      <td>[washington, gov, john, kasich, ohio, tuesday,...</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25836</th>\n",
       "      <td>Good morning. (Want to get California Today by...</td>\n",
       "      <td>[good, morning, want, get, california, today, ...</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25837</th>\n",
       "      <td>« Previous - Next » 300 US Marines To Be Deplo...</td>\n",
       "      <td>[previous, next, us, marines, deployed, russia...</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.623077</td>\n",
       "      <td>0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25838</th>\n",
       "      <td>Perhaps you’ve seen the new TV series whose pi...</td>\n",
       "      <td>[perhaps, youve, seen, new, tv, series, whose,...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25839 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               full_text  \\\n",
       "0      YouTube says no ‘deepfakes’ or ‘birther’ video...   \n",
       "1      FRISCO, Texas — The increasingly bitter disput...   \n",
       "2      Speaking to the country for the first time fro...   \n",
       "3      A professor who teaches climate change classes...   \n",
       "4      The left has a thing for taking babies hostage...   \n",
       "...                                                  ...   \n",
       "25834  Of all the dysfunctions that plague the world’...   \n",
       "25835  WASHINGTON  —   Gov. John Kasich of Ohio on Tu...   \n",
       "25836  Good morning. (Want to get California Today by...   \n",
       "25837  « Previous - Next » 300 US Marines To Be Deplo...   \n",
       "25838  Perhaps you’ve seen the new TV series whose pi...   \n",
       "\n",
       "                                                  tokens  dem_bias  neutral  \\\n",
       "0      [youtube, says, deepfakes, birther, videos, to...  0.350000     0.33   \n",
       "1      [frisco, texas, increasingly, bitter, dispute,...  0.490000     0.12   \n",
       "2      [speaking, country, first, time, oval, office,...  0.730000     0.08   \n",
       "3      [professor, teaches, climate, change, classes,...  0.520000     0.02   \n",
       "4      [left, thing, taking, babies, hostage, perfect...  0.140000     0.01   \n",
       "...                                                  ...       ...      ...   \n",
       "25834  [dysfunctions, plague, worlds, megacities, non...  0.310000     0.06   \n",
       "25835  [washington, gov, john, kasich, ohio, tuesday,...  0.320000     0.38   \n",
       "25836  [good, morning, want, get, california, today, ...  0.190000     0.18   \n",
       "25837  [previous, next, us, marines, deployed, russia...  0.326923     0.05   \n",
       "25838  [perhaps, youve, seen, new, tv, series, whose,...  0.160000     0.01   \n",
       "\n",
       "       rep_bias  cluster  not_misinfo  misinfo  \n",
       "0      0.320000        1         0.55     0.45  \n",
       "1      0.390000        2         0.98     0.02  \n",
       "2      0.190000        2         0.83     0.17  \n",
       "3      0.460000        2         0.48     0.52  \n",
       "4      0.850000        0         0.27     0.73  \n",
       "...         ...      ...          ...      ...  \n",
       "25834  0.630000        0         0.75     0.25  \n",
       "25835  0.300000        1         0.93     0.07  \n",
       "25836  0.630000        0         0.93     0.07  \n",
       "25837  0.623077        0         0.16     0.84  \n",
       "25838  0.830000        0         0.86     0.14  \n",
       "\n",
       "[25839 rows x 8 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally, we'll generate predicted fake news probabilities using our fn classifier.\n",
    "class_frame = vec_frame.join(full_data_clean).drop(columns=['full_text','tokens'])\n",
    "preds = pd.DataFrame(fn_clf.predict_proba(class_frame), columns=['not_misinfo', 'misinfo'])\n",
    "full_data_clean = full_data_clean.join(preds)\n",
    "full_data_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "full_data_clean.to_csv(\"assets/all_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## To-Do:\n",
    "1. Put all NLP stuff in a function, so that we can just pass a column of text and get the clean tokens out.\n",
    "2. (Maybe?) Clean the notebook up into two scripts: one that builds and pickles the models, and one that takes a csv (or set of csvs/excels/whatever), runs all the models, and outputs a csv with all the data needed for the dashboard.\n",
    "3. ??Include headline text in models\n",
    "4. Do some deeper evaluations on the clustering and supervised learning portions (supervised portion is not critical, we have good accuracy right now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}